{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an LSTM-based model to predict stock returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train, validation and test periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date = \"2000-01-01\"\n",
    "train_end_date = \"2014-12-31\"\n",
    "val_start_date = \"2015-01-01\"\n",
    "val_end_date = \"2016-12-31\"\n",
    "test_start_date = \"2017-01-01\"\n",
    "test_end_date = \"2018-12-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nke = pd.read_csv(\"nke.csv\", index_col=0, parse_dates=True)\n",
    "nke[\"return\"] = np.log(nke[\"close\"] / nke[\"close\"].shift())\n",
    "nke[\"label\"] = np.where(nke[\"return\"] > 0, 1, 0)\n",
    "nke.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>return</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>5.875000</td>\n",
       "      <td>5.914063</td>\n",
       "      <td>5.671875</td>\n",
       "      <td>5.687500</td>\n",
       "      <td>9810400</td>\n",
       "      <td>-0.056089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>5.718750</td>\n",
       "      <td>6.046875</td>\n",
       "      <td>5.718750</td>\n",
       "      <td>6.015625</td>\n",
       "      <td>6542400</td>\n",
       "      <td>0.056089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>5.984375</td>\n",
       "      <td>5.984375</td>\n",
       "      <td>5.820313</td>\n",
       "      <td>5.984375</td>\n",
       "      <td>4891200</td>\n",
       "      <td>-0.005208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>5.960938</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.875000</td>\n",
       "      <td>5.984375</td>\n",
       "      <td>3993600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-10</th>\n",
       "      <td>6.015625</td>\n",
       "      <td>6.117188</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.085938</td>\n",
       "      <td>3946400</td>\n",
       "      <td>0.016829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low     close   volume    return  label\n",
       "date                                                                        \n",
       "2000-01-04  5.875000  5.914063  5.671875  5.687500  9810400 -0.056089      0\n",
       "2000-01-05  5.718750  6.046875  5.718750  6.015625  6542400  0.056089      1\n",
       "2000-01-06  5.984375  5.984375  5.820313  5.984375  4891200 -0.005208      0\n",
       "2000-01-07  5.960938  6.000000  5.875000  5.984375  3993600  0.000000      0\n",
       "2000-01-10  6.015625  6.117188  6.000000  6.085938  3946400  0.016829      1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nke.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.513521\n",
       "0    0.486479\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nke.loc[:val_start_date, \"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_mean = nke[\"return\"][:val_start_date].mean()\n",
    "return_std = nke[\"return\"][:val_start_date].std()\n",
    "nke[\"norm_return\"] = (nke[\"return\"] - return_mean) / return_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>return</th>\n",
       "      <th>label</th>\n",
       "      <th>norm_return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>5.875000</td>\n",
       "      <td>5.914063</td>\n",
       "      <td>5.671875</td>\n",
       "      <td>5.687500</td>\n",
       "      <td>9810400</td>\n",
       "      <td>-0.056089</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.861218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>5.718750</td>\n",
       "      <td>6.046875</td>\n",
       "      <td>5.718750</td>\n",
       "      <td>6.015625</td>\n",
       "      <td>6542400</td>\n",
       "      <td>0.056089</td>\n",
       "      <td>1</td>\n",
       "      <td>2.805549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>5.984375</td>\n",
       "      <td>5.984375</td>\n",
       "      <td>5.820313</td>\n",
       "      <td>5.984375</td>\n",
       "      <td>4891200</td>\n",
       "      <td>-0.005208</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.290936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>5.960938</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.875000</td>\n",
       "      <td>5.984375</td>\n",
       "      <td>3993600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.027834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-10</th>\n",
       "      <td>6.015625</td>\n",
       "      <td>6.117188</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.085938</td>\n",
       "      <td>3946400</td>\n",
       "      <td>0.016829</td>\n",
       "      <td>1</td>\n",
       "      <td>0.822288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low     close   volume    return  label  \\\n",
       "date                                                                           \n",
       "2000-01-04  5.875000  5.914063  5.671875  5.687500  9810400 -0.056089      0   \n",
       "2000-01-05  5.718750  6.046875  5.718750  6.015625  6542400  0.056089      1   \n",
       "2000-01-06  5.984375  5.984375  5.820313  5.984375  4891200 -0.005208      0   \n",
       "2000-01-07  5.960938  6.000000  5.875000  5.984375  3993600  0.000000      0   \n",
       "2000-01-10  6.015625  6.117188  6.000000  6.085938  3946400  0.016829      1   \n",
       "\n",
       "            norm_return  \n",
       "date                     \n",
       "2000-01-04    -2.861218  \n",
       "2000-01-05     2.805549  \n",
       "2000-01-06    -0.290936  \n",
       "2000-01-07    -0.027834  \n",
       "2000-01-10     0.822288  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nke.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_start_idx = nke.index.get_loc(val_start_date, method=\"bfill\")\n",
    "test_start_idx = nke.index.get_loc(test_start_date, method=\"bfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = TimeseriesGenerator(nke[[\"norm_return\"]].values, nke[[\"label\"]].values,\n",
    "                                      length=50, batch_size=64, end_index=val_start_idx-1)\n",
    "val_generator = TimeseriesGenerator(nke[[\"norm_return\"]].values, nke[[\"label\"]].values,\n",
    "                                    length=50, batch_size=64, start_index=val_start_idx,\n",
    "                                    end_index=test_start_idx-1)\n",
    "test_generator = TimeseriesGenerator(nke[[\"norm_return\"]].values, nke[[\"label\"]].values,\n",
    "                                     length=50, batch_size=64, start_index=test_start_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `model_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(params):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.CuDNNLSTM(params[\"lstm_size\"], input_shape=(50, 1)))\n",
    "    model.add(tf.keras.layers.Dropout(params[\"dropout\"]))\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(params[\"learning_rate\"]),\n",
    "                  loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "    history = model.fit_generator(train_generator, validation_data=val_generator, callbacks=callbacks,\n",
    "                                  epochs=200, verbose=0).history\n",
    "    return (history, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(model_fn, p, n, search_dir=None):\n",
    "    results = []\n",
    "    os.mkdir(search_dir)\n",
    "    best_model_path = os.path.join(search_dir, \"best_model.h5\")\n",
    "    results_path = os.path.join(search_dir, \"results.csv\")\n",
    "    for i in range(n):\n",
    "        params = {k: v[np.random.randint(len(v))] for k, v in p.items()}\n",
    "        history, model = model_fn(params)\n",
    "        epochs = np.argmin(history[\"val_loss\"]) + 1\n",
    "        result = {k: v[epochs - 1] for k, v in history.items()}\n",
    "        params[\"epochs\"] = epochs\n",
    "        if i == 0:\n",
    "            best_val_acc = result[\"val_acc\"]\n",
    "            model.save(best_model_path)\n",
    "        if result[\"val_acc\"] > best_val_acc:\n",
    "            best_val_acc = result[\"val_acc\"]\n",
    "            model.save(best_model_path)\n",
    "        result = {**params, **result}\n",
    "        results.append(result)\n",
    "        tf.keras.backend.clear_session()\n",
    "        print(f\"iteration {i + 1} – {', '.join(f'{k}:{v:.4g}' for k, v in result.items())}\")\n",
    "    best_model = tf.keras.models.load_model(best_model_path)\n",
    "    results = pd.DataFrame(results)\n",
    "    results.to_csv(results_path)\n",
    "    return (results, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\"lstm_size\": np.linspace(50, 200, 16, dtype=int),\n",
    "     \"dropout\": np.linspace(0, 0.4, 9),\n",
    "     \"learning_rate\": np.linspace(0.004, 0.01, 13)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0712 12:55:27.146746 17652 deprecation.py:506] From C:\\Users\\jack_\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0712 12:55:27.282870 17652 deprecation.py:323] From C:\\Users\\jack_\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 – lstm_size:100, dropout:0.15, learning_rate:0.0065, epochs:6, loss:0.6916, acc:0.5287, val_loss:0.6895, val_acc:0.5242\n",
      "iteration 2 – lstm_size:60, dropout:0.3, learning_rate:0.0055, epochs:10, loss:0.6902, acc:0.5352, val_loss:0.6896, val_acc:0.5088\n",
      "iteration 3 – lstm_size:90, dropout:0.1, learning_rate:0.01, epochs:5, loss:0.6909, acc:0.521, val_loss:0.6882, val_acc:0.5463\n",
      "iteration 4 – lstm_size:180, dropout:0.4, learning_rate:0.007, epochs:9, loss:0.691, acc:0.5317, val_loss:0.6873, val_acc:0.5264\n",
      "iteration 5 – lstm_size:90, dropout:0.3, learning_rate:0.005, epochs:10, loss:0.6907, acc:0.5239, val_loss:0.6892, val_acc:0.522\n",
      "iteration 6 – lstm_size:60, dropout:0.15, learning_rate:0.009, epochs:6, loss:0.6908, acc:0.5215, val_loss:0.6915, val_acc:0.5176\n",
      "iteration 7 – lstm_size:180, dropout:0.05, learning_rate:0.004, epochs:3, loss:0.691, acc:0.5314, val_loss:0.6904, val_acc:0.5176\n",
      "iteration 8 – lstm_size:80, dropout:0.2, learning_rate:0.01, epochs:1, loss:0.6932, acc:0.5191, val_loss:0.6901, val_acc:0.5308\n",
      "iteration 9 – lstm_size:130, dropout:0, learning_rate:0.0055, epochs:2, loss:0.6921, acc:0.5212, val_loss:0.6888, val_acc:0.511\n",
      "iteration 10 – lstm_size:100, dropout:0.3, learning_rate:0.009, epochs:8, loss:0.691, acc:0.5282, val_loss:0.6877, val_acc:0.5242\n",
      "iteration 11 – lstm_size:170, dropout:0.4, learning_rate:0.007, epochs:3, loss:0.6924, acc:0.5177, val_loss:0.6857, val_acc:0.5374\n",
      "iteration 12 – lstm_size:70, dropout:0.35, learning_rate:0.0065, epochs:8, loss:0.6907, acc:0.5298, val_loss:0.6882, val_acc:0.5352\n",
      "iteration 13 – lstm_size:140, dropout:0, learning_rate:0.0045, epochs:3, loss:0.6955, acc:0.5293, val_loss:0.6897, val_acc:0.522\n",
      "iteration 14 – lstm_size:80, dropout:0.35, learning_rate:0.0065, epochs:9, loss:0.6916, acc:0.5258, val_loss:0.6905, val_acc:0.5176\n",
      "iteration 15 – lstm_size:60, dropout:0.2, learning_rate:0.005, epochs:2, loss:0.6912, acc:0.5271, val_loss:0.6891, val_acc:0.5176\n",
      "iteration 16 – lstm_size:60, dropout:0.15, learning_rate:0.0095, epochs:5, loss:0.691, acc:0.5279, val_loss:0.6904, val_acc:0.5176\n",
      "iteration 17 – lstm_size:180, dropout:0.15, learning_rate:0.004, epochs:1, loss:0.6936, acc:0.5193, val_loss:0.6918, val_acc:0.489\n",
      "iteration 18 – lstm_size:50, dropout:0, learning_rate:0.0095, epochs:7, loss:0.6917, acc:0.5202, val_loss:0.6912, val_acc:0.5132\n",
      "iteration 19 – lstm_size:60, dropout:0.3, learning_rate:0.0085, epochs:7, loss:0.691, acc:0.5255, val_loss:0.6894, val_acc:0.5242\n",
      "iteration 20 – lstm_size:50, dropout:0.4, learning_rate:0.005, epochs:2, loss:0.692, acc:0.5253, val_loss:0.6881, val_acc:0.5286\n",
      "iteration 21 – lstm_size:70, dropout:0, learning_rate:0.007, epochs:11, loss:0.6903, acc:0.5347, val_loss:0.6884, val_acc:0.5374\n",
      "iteration 22 – lstm_size:120, dropout:0.4, learning_rate:0.007, epochs:2, loss:0.6938, acc:0.5142, val_loss:0.6883, val_acc:0.533\n",
      "iteration 23 – lstm_size:80, dropout:0.3, learning_rate:0.0065, epochs:5, loss:0.6914, acc:0.5223, val_loss:0.6915, val_acc:0.5044\n",
      "iteration 24 – lstm_size:190, dropout:0.05, learning_rate:0.0065, epochs:6, loss:0.6918, acc:0.5296, val_loss:0.6886, val_acc:0.5198\n",
      "iteration 25 – lstm_size:130, dropout:0.3, learning_rate:0.0065, epochs:7, loss:0.6902, acc:0.5298, val_loss:0.6868, val_acc:0.5066\n",
      "iteration 26 – lstm_size:90, dropout:0.15, learning_rate:0.0095, epochs:12, loss:0.6903, acc:0.5349, val_loss:0.6891, val_acc:0.5286\n",
      "iteration 27 – lstm_size:140, dropout:0.05, learning_rate:0.008, epochs:3, loss:0.6925, acc:0.5263, val_loss:0.69, val_acc:0.5242\n",
      "iteration 28 – lstm_size:100, dropout:0.2, learning_rate:0.0085, epochs:2, loss:0.6917, acc:0.5263, val_loss:0.689, val_acc:0.5264\n",
      "iteration 29 – lstm_size:190, dropout:0.2, learning_rate:0.008, epochs:13, loss:0.6915, acc:0.5228, val_loss:0.6863, val_acc:0.5396\n",
      "iteration 30 – lstm_size:120, dropout:0.1, learning_rate:0.004, epochs:1, loss:0.6938, acc:0.5253, val_loss:0.6926, val_acc:0.511\n",
      "iteration 31 – lstm_size:90, dropout:0.3, learning_rate:0.008, epochs:6, loss:0.6911, acc:0.5285, val_loss:0.6868, val_acc:0.5396\n",
      "iteration 32 – lstm_size:180, dropout:0.15, learning_rate:0.0075, epochs:5, loss:0.6919, acc:0.5304, val_loss:0.6905, val_acc:0.522\n",
      "iteration 33 – lstm_size:80, dropout:0.25, learning_rate:0.007, epochs:4, loss:0.6922, acc:0.5287, val_loss:0.6898, val_acc:0.5242\n",
      "iteration 34 – lstm_size:190, dropout:0.05, learning_rate:0.008, epochs:3, loss:0.6919, acc:0.5347, val_loss:0.6907, val_acc:0.533\n",
      "iteration 35 – lstm_size:140, dropout:0.35, learning_rate:0.004, epochs:1, loss:0.6937, acc:0.5226, val_loss:0.6886, val_acc:0.5286\n",
      "iteration 36 – lstm_size:60, dropout:0.4, learning_rate:0.0075, epochs:7, loss:0.6946, acc:0.5207, val_loss:0.6872, val_acc:0.5066\n",
      "iteration 37 – lstm_size:190, dropout:0.1, learning_rate:0.0075, epochs:7, loss:0.6912, acc:0.5317, val_loss:0.6894, val_acc:0.5176\n",
      "iteration 38 – lstm_size:150, dropout:0.35, learning_rate:0.0045, epochs:6, loss:0.6911, acc:0.5244, val_loss:0.6896, val_acc:0.5066\n",
      "iteration 39 – lstm_size:120, dropout:0.2, learning_rate:0.01, epochs:2, loss:0.6939, acc:0.5177, val_loss:0.6868, val_acc:0.5198\n",
      "iteration 40 – lstm_size:60, dropout:0.35, learning_rate:0.009, epochs:6, loss:0.6915, acc:0.5269, val_loss:0.6907, val_acc:0.5088\n",
      "iteration 41 – lstm_size:110, dropout:0.15, learning_rate:0.01, epochs:6, loss:0.6912, acc:0.5309, val_loss:0.6856, val_acc:0.5463\n",
      "iteration 42 – lstm_size:50, dropout:0.35, learning_rate:0.008, epochs:7, loss:0.6914, acc:0.5282, val_loss:0.6902, val_acc:0.5176\n",
      "iteration 43 – lstm_size:80, dropout:0.2, learning_rate:0.0045, epochs:1, loss:0.6937, acc:0.5191, val_loss:0.6895, val_acc:0.533\n",
      "iteration 44 – lstm_size:80, dropout:0.15, learning_rate:0.009, epochs:9, loss:0.6908, acc:0.5344, val_loss:0.6899, val_acc:0.5352\n",
      "iteration 45 – lstm_size:160, dropout:0.15, learning_rate:0.0095, epochs:11, loss:0.691, acc:0.5382, val_loss:0.6898, val_acc:0.5176\n",
      "iteration 46 – lstm_size:180, dropout:0.05, learning_rate:0.0075, epochs:6, loss:0.6918, acc:0.5263, val_loss:0.6885, val_acc:0.5396\n",
      "iteration 47 – lstm_size:90, dropout:0.15, learning_rate:0.0085, epochs:2, loss:0.6923, acc:0.5247, val_loss:0.6907, val_acc:0.511\n",
      "iteration 48 – lstm_size:190, dropout:0.3, learning_rate:0.0045, epochs:2, loss:0.6926, acc:0.514, val_loss:0.6894, val_acc:0.5396\n",
      "iteration 49 – lstm_size:110, dropout:0.15, learning_rate:0.004, epochs:5, loss:0.6915, acc:0.5314, val_loss:0.6873, val_acc:0.5286\n",
      "iteration 50 – lstm_size:150, dropout:0.15, learning_rate:0.0045, epochs:4, loss:0.6918, acc:0.5274, val_loss:0.6916, val_acc:0.5132\n",
      "iteration 51 – lstm_size:120, dropout:0.1, learning_rate:0.008, epochs:3, loss:0.6913, acc:0.5218, val_loss:0.6893, val_acc:0.5286\n",
      "iteration 52 – lstm_size:120, dropout:0.35, learning_rate:0.0065, epochs:4, loss:0.6922, acc:0.5153, val_loss:0.6901, val_acc:0.5176\n",
      "iteration 53 – lstm_size:50, dropout:0.4, learning_rate:0.0045, epochs:2, loss:0.6923, acc:0.5263, val_loss:0.6883, val_acc:0.5132\n",
      "iteration 54 – lstm_size:160, dropout:0.25, learning_rate:0.008, epochs:2, loss:0.6938, acc:0.518, val_loss:0.6874, val_acc:0.5441\n",
      "iteration 55 – lstm_size:90, dropout:0.1, learning_rate:0.0045, epochs:19, loss:0.6864, acc:0.5349, val_loss:0.6877, val_acc:0.5463\n",
      "iteration 56 – lstm_size:90, dropout:0.05, learning_rate:0.005, epochs:2, loss:0.6919, acc:0.5236, val_loss:0.6918, val_acc:0.4978\n",
      "iteration 57 – lstm_size:130, dropout:0.25, learning_rate:0.007, epochs:2, loss:0.6928, acc:0.5314, val_loss:0.6874, val_acc:0.5374\n",
      "iteration 58 – lstm_size:140, dropout:0.25, learning_rate:0.0075, epochs:1, loss:0.6942, acc:0.5177, val_loss:0.6882, val_acc:0.5066\n",
      "iteration 59 – lstm_size:60, dropout:0.25, learning_rate:0.0045, epochs:4, loss:0.6911, acc:0.5317, val_loss:0.6877, val_acc:0.5154\n",
      "iteration 60 – lstm_size:200, dropout:0.1, learning_rate:0.0055, epochs:1, loss:0.6944, acc:0.5193, val_loss:0.6923, val_acc:0.489\n",
      "iteration 61 – lstm_size:120, dropout:0.35, learning_rate:0.0075, epochs:7, loss:0.6917, acc:0.5202, val_loss:0.6892, val_acc:0.5264\n",
      "iteration 62 – lstm_size:60, dropout:0.05, learning_rate:0.0085, epochs:3, loss:0.6916, acc:0.5317, val_loss:0.6895, val_acc:0.5242\n",
      "iteration 63 – lstm_size:200, dropout:0.4, learning_rate:0.0055, epochs:5, loss:0.6915, acc:0.529, val_loss:0.6907, val_acc:0.5198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 64 – lstm_size:60, dropout:0.3, learning_rate:0.004, epochs:12, loss:0.6901, acc:0.5301, val_loss:0.6898, val_acc:0.5242\n",
      "iteration 65 – lstm_size:50, dropout:0.05, learning_rate:0.0095, epochs:8, loss:0.6902, acc:0.5322, val_loss:0.6906, val_acc:0.5132\n",
      "iteration 66 – lstm_size:180, dropout:0.2, learning_rate:0.0045, epochs:1, loss:0.6934, acc:0.5228, val_loss:0.6915, val_acc:0.5154\n",
      "iteration 67 – lstm_size:80, dropout:0.3, learning_rate:0.009, epochs:1, loss:0.6945, acc:0.511, val_loss:0.689, val_acc:0.5242\n",
      "iteration 68 – lstm_size:70, dropout:0.1, learning_rate:0.0045, epochs:3, loss:0.6917, acc:0.5266, val_loss:0.6903, val_acc:0.522\n",
      "iteration 69 – lstm_size:180, dropout:0.35, learning_rate:0.0045, epochs:5, loss:0.6958, acc:0.5054, val_loss:0.6926, val_acc:0.489\n",
      "iteration 70 – lstm_size:180, dropout:0.1, learning_rate:0.009, epochs:4, loss:0.696, acc:0.5177, val_loss:0.6905, val_acc:0.5044\n",
      "iteration 71 – lstm_size:200, dropout:0.15, learning_rate:0.0075, epochs:3, loss:0.6926, acc:0.5167, val_loss:0.6916, val_acc:0.5044\n",
      "iteration 72 – lstm_size:70, dropout:0.35, learning_rate:0.0095, epochs:8, loss:0.6904, acc:0.5266, val_loss:0.6882, val_acc:0.533\n",
      "iteration 73 – lstm_size:170, dropout:0.3, learning_rate:0.0095, epochs:1, loss:0.6943, acc:0.5177, val_loss:0.6884, val_acc:0.5286\n",
      "iteration 74 – lstm_size:120, dropout:0, learning_rate:0.0045, epochs:1, loss:0.6922, acc:0.5212, val_loss:0.6922, val_acc:0.5066\n",
      "iteration 75 – lstm_size:100, dropout:0.1, learning_rate:0.004, epochs:4, loss:0.6917, acc:0.5269, val_loss:0.6896, val_acc:0.5308\n",
      "iteration 76 – lstm_size:110, dropout:0.4, learning_rate:0.004, epochs:14, loss:0.6898, acc:0.5344, val_loss:0.6876, val_acc:0.5308\n",
      "iteration 77 – lstm_size:90, dropout:0.1, learning_rate:0.0055, epochs:6, loss:0.6912, acc:0.5279, val_loss:0.6896, val_acc:0.5066\n",
      "iteration 78 – lstm_size:90, dropout:0.1, learning_rate:0.0065, epochs:4, loss:0.6915, acc:0.5236, val_loss:0.6893, val_acc:0.533\n",
      "iteration 79 – lstm_size:110, dropout:0.25, learning_rate:0.0045, epochs:6, loss:0.691, acc:0.5325, val_loss:0.6862, val_acc:0.5419\n",
      "iteration 80 – lstm_size:150, dropout:0.4, learning_rate:0.004, epochs:6, loss:0.6912, acc:0.5293, val_loss:0.6892, val_acc:0.5132\n",
      "iteration 81 – lstm_size:90, dropout:0, learning_rate:0.0085, epochs:1, loss:0.694, acc:0.521, val_loss:0.6874, val_acc:0.5352\n",
      "iteration 82 – lstm_size:80, dropout:0.1, learning_rate:0.01, epochs:3, loss:0.692, acc:0.5247, val_loss:0.6896, val_acc:0.5242\n",
      "iteration 83 – lstm_size:170, dropout:0.1, learning_rate:0.004, epochs:7, loss:0.6915, acc:0.5244, val_loss:0.6887, val_acc:0.522\n",
      "iteration 84 – lstm_size:60, dropout:0.1, learning_rate:0.006, epochs:13, loss:0.6884, acc:0.54, val_loss:0.6888, val_acc:0.5154\n",
      "iteration 85 – lstm_size:130, dropout:0.35, learning_rate:0.0045, epochs:4, loss:0.691, acc:0.5287, val_loss:0.6888, val_acc:0.5286\n",
      "iteration 86 – lstm_size:180, dropout:0, learning_rate:0.0085, epochs:2, loss:0.6918, acc:0.5172, val_loss:0.6914, val_acc:0.5176\n",
      "iteration 87 – lstm_size:50, dropout:0.1, learning_rate:0.0055, epochs:7, loss:0.6915, acc:0.5296, val_loss:0.6898, val_acc:0.522\n",
      "iteration 88 – lstm_size:190, dropout:0.2, learning_rate:0.008, epochs:5, loss:0.692, acc:0.5269, val_loss:0.6928, val_acc:0.4934\n",
      "iteration 89 – lstm_size:160, dropout:0.15, learning_rate:0.008, epochs:7, loss:0.6913, acc:0.5244, val_loss:0.6913, val_acc:0.511\n",
      "iteration 90 – lstm_size:200, dropout:0.4, learning_rate:0.0055, epochs:3, loss:0.6916, acc:0.5285, val_loss:0.6864, val_acc:0.5485\n",
      "iteration 91 – lstm_size:120, dropout:0.4, learning_rate:0.0065, epochs:4, loss:0.6919, acc:0.5234, val_loss:0.6904, val_acc:0.522\n",
      "iteration 92 – lstm_size:130, dropout:0.15, learning_rate:0.004, epochs:3, loss:0.6914, acc:0.5277, val_loss:0.6896, val_acc:0.522\n",
      "iteration 93 – lstm_size:150, dropout:0, learning_rate:0.009, epochs:4, loss:0.694, acc:0.5099, val_loss:0.6912, val_acc:0.5044\n",
      "iteration 94 – lstm_size:70, dropout:0.05, learning_rate:0.0095, epochs:4, loss:0.6913, acc:0.5261, val_loss:0.6891, val_acc:0.5176\n",
      "iteration 95 – lstm_size:140, dropout:0, learning_rate:0.0045, epochs:3, loss:0.6912, acc:0.5274, val_loss:0.6878, val_acc:0.5286\n",
      "iteration 96 – lstm_size:180, dropout:0.2, learning_rate:0.009, epochs:3, loss:0.6929, acc:0.5169, val_loss:0.6891, val_acc:0.5132\n",
      "iteration 97 – lstm_size:100, dropout:0.35, learning_rate:0.008, epochs:6, loss:0.6917, acc:0.5218, val_loss:0.6899, val_acc:0.5176\n",
      "iteration 98 – lstm_size:50, dropout:0.15, learning_rate:0.009, epochs:7, loss:0.6907, acc:0.5411, val_loss:0.69, val_acc:0.5374\n",
      "iteration 99 – lstm_size:150, dropout:0.35, learning_rate:0.005, epochs:2, loss:0.6928, acc:0.5287, val_loss:0.6908, val_acc:0.5154\n",
      "iteration 100 – lstm_size:60, dropout:0, learning_rate:0.009, epochs:4, loss:0.6912, acc:0.5279, val_loss:0.6922, val_acc:0.511\n",
      "iteration 101 – lstm_size:200, dropout:0, learning_rate:0.009, epochs:7, loss:0.6907, acc:0.5244, val_loss:0.6899, val_acc:0.5198\n",
      "iteration 102 – lstm_size:50, dropout:0.1, learning_rate:0.007, epochs:3, loss:0.6921, acc:0.5188, val_loss:0.6901, val_acc:0.5088\n",
      "iteration 103 – lstm_size:100, dropout:0.2, learning_rate:0.009, epochs:7, loss:0.6905, acc:0.5296, val_loss:0.6868, val_acc:0.5441\n",
      "iteration 104 – lstm_size:140, dropout:0.25, learning_rate:0.01, epochs:6, loss:0.6909, acc:0.5244, val_loss:0.6855, val_acc:0.5529\n",
      "iteration 105 – lstm_size:180, dropout:0.3, learning_rate:0.01, epochs:5, loss:0.6929, acc:0.5199, val_loss:0.6905, val_acc:0.5396\n",
      "iteration 106 – lstm_size:70, dropout:0.05, learning_rate:0.0045, epochs:6, loss:0.6916, acc:0.5277, val_loss:0.6917, val_acc:0.489\n",
      "iteration 107 – lstm_size:80, dropout:0.05, learning_rate:0.0075, epochs:10, loss:0.6907, acc:0.5355, val_loss:0.6873, val_acc:0.522\n",
      "iteration 108 – lstm_size:200, dropout:0.2, learning_rate:0.01, epochs:1, loss:0.6978, acc:0.5086, val_loss:0.6866, val_acc:0.5066\n",
      "iteration 109 – lstm_size:170, dropout:0.25, learning_rate:0.0085, epochs:11, loss:0.6918, acc:0.5172, val_loss:0.6885, val_acc:0.5396\n",
      "iteration 110 – lstm_size:130, dropout:0.4, learning_rate:0.006, epochs:10, loss:0.691, acc:0.5255, val_loss:0.6891, val_acc:0.5242\n",
      "iteration 111 – lstm_size:100, dropout:0, learning_rate:0.005, epochs:3, loss:0.6917, acc:0.5231, val_loss:0.6918, val_acc:0.5022\n",
      "iteration 112 – lstm_size:120, dropout:0.25, learning_rate:0.006, epochs:4, loss:0.6913, acc:0.5282, val_loss:0.6908, val_acc:0.4912\n",
      "iteration 113 – lstm_size:110, dropout:0.15, learning_rate:0.01, epochs:3, loss:0.6924, acc:0.5277, val_loss:0.6923, val_acc:0.5066\n",
      "iteration 114 – lstm_size:90, dropout:0.05, learning_rate:0.009, epochs:8, loss:0.6913, acc:0.5349, val_loss:0.6898, val_acc:0.533\n",
      "iteration 115 – lstm_size:140, dropout:0.2, learning_rate:0.008, epochs:1, loss:0.6959, acc:0.5159, val_loss:0.6881, val_acc:0.522\n",
      "iteration 116 – lstm_size:80, dropout:0.35, learning_rate:0.0045, epochs:2, loss:0.6929, acc:0.5185, val_loss:0.6932, val_acc:0.5\n",
      "iteration 117 – lstm_size:180, dropout:0.35, learning_rate:0.0095, epochs:8, loss:0.6915, acc:0.5218, val_loss:0.689, val_acc:0.5286\n",
      "iteration 118 – lstm_size:80, dropout:0.15, learning_rate:0.0095, epochs:6, loss:0.6909, acc:0.5271, val_loss:0.6872, val_acc:0.5176\n",
      "iteration 119 – lstm_size:60, dropout:0.2, learning_rate:0.008, epochs:4, loss:0.6914, acc:0.5277, val_loss:0.6908, val_acc:0.5176\n",
      "iteration 120 – lstm_size:170, dropout:0.4, learning_rate:0.0065, epochs:1, loss:0.695, acc:0.5043, val_loss:0.6887, val_acc:0.5441\n",
      "iteration 121 – lstm_size:200, dropout:0.3, learning_rate:0.005, epochs:1, loss:0.6938, acc:0.5148, val_loss:0.6881, val_acc:0.5088\n",
      "iteration 122 – lstm_size:120, dropout:0, learning_rate:0.0055, epochs:1, loss:0.6933, acc:0.5196, val_loss:0.6881, val_acc:0.5419\n",
      "iteration 123 – lstm_size:60, dropout:0.35, learning_rate:0.009, epochs:8, loss:0.6919, acc:0.5212, val_loss:0.6907, val_acc:0.5198\n",
      "iteration 124 – lstm_size:170, dropout:0.3, learning_rate:0.006, epochs:3, loss:0.6929, acc:0.5177, val_loss:0.6875, val_acc:0.5132\n",
      "iteration 125 – lstm_size:160, dropout:0.25, learning_rate:0.0065, epochs:9, loss:0.6909, acc:0.5298, val_loss:0.6884, val_acc:0.5242\n",
      "iteration 126 – lstm_size:200, dropout:0.2, learning_rate:0.01, epochs:10, loss:0.6911, acc:0.536, val_loss:0.6843, val_acc:0.5595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 127 – lstm_size:200, dropout:0, learning_rate:0.01, epochs:3, loss:0.6919, acc:0.5282, val_loss:0.6893, val_acc:0.5242\n",
      "iteration 128 – lstm_size:80, dropout:0.05, learning_rate:0.0095, epochs:6, loss:0.6912, acc:0.5253, val_loss:0.6896, val_acc:0.5176\n",
      "iteration 129 – lstm_size:60, dropout:0.1, learning_rate:0.004, epochs:2, loss:0.6923, acc:0.5274, val_loss:0.6915, val_acc:0.5088\n",
      "iteration 130 – lstm_size:70, dropout:0, learning_rate:0.0045, epochs:5, loss:0.6913, acc:0.5328, val_loss:0.6895, val_acc:0.5242\n",
      "iteration 131 – lstm_size:90, dropout:0.35, learning_rate:0.01, epochs:3, loss:0.6912, acc:0.5185, val_loss:0.6918, val_acc:0.5022\n",
      "iteration 132 – lstm_size:90, dropout:0.4, learning_rate:0.0055, epochs:7, loss:0.6913, acc:0.5236, val_loss:0.6876, val_acc:0.5088\n",
      "iteration 133 – lstm_size:150, dropout:0.35, learning_rate:0.009, epochs:10, loss:0.692, acc:0.5312, val_loss:0.6886, val_acc:0.5374\n",
      "iteration 134 – lstm_size:160, dropout:0.4, learning_rate:0.0075, epochs:2, loss:0.6929, acc:0.5132, val_loss:0.6889, val_acc:0.5286\n",
      "iteration 135 – lstm_size:70, dropout:0, learning_rate:0.006, epochs:1, loss:0.6942, acc:0.5159, val_loss:0.6904, val_acc:0.5286\n",
      "iteration 136 – lstm_size:70, dropout:0.15, learning_rate:0.0065, epochs:2, loss:0.6921, acc:0.5242, val_loss:0.6914, val_acc:0.5198\n",
      "iteration 137 – lstm_size:80, dropout:0.35, learning_rate:0.009, epochs:2, loss:0.6928, acc:0.5218, val_loss:0.6905, val_acc:0.511\n",
      "iteration 138 – lstm_size:120, dropout:0.1, learning_rate:0.008, epochs:1, loss:0.6953, acc:0.5191, val_loss:0.693, val_acc:0.5044\n",
      "iteration 139 – lstm_size:50, dropout:0.25, learning_rate:0.0045, epochs:1, loss:0.6929, acc:0.5105, val_loss:0.688, val_acc:0.5242\n",
      "iteration 140 – lstm_size:170, dropout:0.25, learning_rate:0.0045, epochs:7, loss:0.6914, acc:0.5263, val_loss:0.6894, val_acc:0.5154\n",
      "iteration 141 – lstm_size:130, dropout:0.1, learning_rate:0.006, epochs:5, loss:0.6918, acc:0.5261, val_loss:0.6897, val_acc:0.5242\n",
      "iteration 142 – lstm_size:80, dropout:0, learning_rate:0.008, epochs:7, loss:0.6907, acc:0.5261, val_loss:0.6909, val_acc:0.5352\n",
      "iteration 143 – lstm_size:70, dropout:0.2, learning_rate:0.0075, epochs:6, loss:0.6909, acc:0.5339, val_loss:0.6877, val_acc:0.522\n",
      "iteration 144 – lstm_size:80, dropout:0.25, learning_rate:0.01, epochs:2, loss:0.6919, acc:0.5242, val_loss:0.693, val_acc:0.5044\n",
      "iteration 145 – lstm_size:100, dropout:0.05, learning_rate:0.008, epochs:16, loss:0.6885, acc:0.5298, val_loss:0.6859, val_acc:0.5286\n",
      "iteration 146 – lstm_size:200, dropout:0.2, learning_rate:0.006, epochs:6, loss:0.692, acc:0.5301, val_loss:0.691, val_acc:0.5264\n",
      "iteration 147 – lstm_size:170, dropout:0.4, learning_rate:0.009, epochs:12, loss:0.6916, acc:0.5169, val_loss:0.6901, val_acc:0.5066\n",
      "iteration 148 – lstm_size:80, dropout:0.05, learning_rate:0.004, epochs:3, loss:0.6916, acc:0.5282, val_loss:0.689, val_acc:0.5286\n",
      "iteration 149 – lstm_size:190, dropout:0.25, learning_rate:0.01, epochs:4, loss:0.6926, acc:0.5239, val_loss:0.6873, val_acc:0.522\n",
      "iteration 150 – lstm_size:170, dropout:0.35, learning_rate:0.008, epochs:1, loss:0.7004, acc:0.5043, val_loss:0.6902, val_acc:0.511\n",
      "iteration 151 – lstm_size:60, dropout:0.2, learning_rate:0.0075, epochs:2, loss:0.6918, acc:0.525, val_loss:0.6914, val_acc:0.5\n",
      "iteration 152 – lstm_size:110, dropout:0.15, learning_rate:0.0055, epochs:5, loss:0.6916, acc:0.5298, val_loss:0.6898, val_acc:0.5242\n",
      "iteration 153 – lstm_size:180, dropout:0.05, learning_rate:0.005, epochs:11, loss:0.691, acc:0.5355, val_loss:0.6912, val_acc:0.5088\n",
      "iteration 154 – lstm_size:170, dropout:0.25, learning_rate:0.0045, epochs:3, loss:0.6924, acc:0.5244, val_loss:0.6885, val_acc:0.5242\n",
      "iteration 155 – lstm_size:70, dropout:0.1, learning_rate:0.0095, epochs:1, loss:0.6934, acc:0.5185, val_loss:0.6892, val_acc:0.5176\n",
      "iteration 156 – lstm_size:80, dropout:0.4, learning_rate:0.004, epochs:2, loss:0.6931, acc:0.5204, val_loss:0.693, val_acc:0.5022\n",
      "iteration 157 – lstm_size:130, dropout:0.05, learning_rate:0.0095, epochs:5, loss:0.6914, acc:0.5212, val_loss:0.6879, val_acc:0.5264\n",
      "iteration 158 – lstm_size:150, dropout:0.2, learning_rate:0.006, epochs:1, loss:0.6941, acc:0.5159, val_loss:0.6903, val_acc:0.5308\n",
      "iteration 159 – lstm_size:50, dropout:0.35, learning_rate:0.009, epochs:4, loss:0.6916, acc:0.521, val_loss:0.6882, val_acc:0.5374\n",
      "iteration 160 – lstm_size:200, dropout:0.2, learning_rate:0.005, epochs:2, loss:0.6924, acc:0.5261, val_loss:0.6877, val_acc:0.5286\n",
      "iteration 161 – lstm_size:130, dropout:0.1, learning_rate:0.0055, epochs:2, loss:0.692, acc:0.5255, val_loss:0.6878, val_acc:0.5286\n",
      "iteration 162 – lstm_size:120, dropout:0.4, learning_rate:0.005, epochs:5, loss:0.692, acc:0.5263, val_loss:0.689, val_acc:0.5198\n",
      "iteration 163 – lstm_size:50, dropout:0.35, learning_rate:0.0045, epochs:1, loss:0.6943, acc:0.5089, val_loss:0.6905, val_acc:0.511\n",
      "iteration 164 – lstm_size:60, dropout:0.25, learning_rate:0.0085, epochs:8, loss:0.691, acc:0.5325, val_loss:0.6907, val_acc:0.5066\n",
      "iteration 165 – lstm_size:80, dropout:0.3, learning_rate:0.007, epochs:9, loss:0.6904, acc:0.5301, val_loss:0.6877, val_acc:0.5154\n",
      "iteration 166 – lstm_size:190, dropout:0.15, learning_rate:0.0095, epochs:3, loss:0.692, acc:0.5306, val_loss:0.6884, val_acc:0.5419\n",
      "iteration 167 – lstm_size:140, dropout:0.4, learning_rate:0.006, epochs:10, loss:0.6901, acc:0.5293, val_loss:0.6878, val_acc:0.5286\n",
      "iteration 168 – lstm_size:190, dropout:0.35, learning_rate:0.0045, epochs:3, loss:0.692, acc:0.5266, val_loss:0.689, val_acc:0.522\n",
      "iteration 169 – lstm_size:190, dropout:0.4, learning_rate:0.009, epochs:8, loss:0.6993, acc:0.5008, val_loss:0.6893, val_acc:0.5044\n",
      "iteration 170 – lstm_size:170, dropout:0.15, learning_rate:0.0085, epochs:4, loss:0.6924, acc:0.5196, val_loss:0.6881, val_acc:0.5264\n",
      "iteration 171 – lstm_size:120, dropout:0.3, learning_rate:0.0065, epochs:7, loss:0.6907, acc:0.5333, val_loss:0.6898, val_acc:0.533\n",
      "iteration 172 – lstm_size:200, dropout:0.25, learning_rate:0.0045, epochs:3, loss:0.6915, acc:0.5341, val_loss:0.6874, val_acc:0.5088\n",
      "iteration 173 – lstm_size:50, dropout:0.25, learning_rate:0.0085, epochs:9, loss:0.6905, acc:0.5371, val_loss:0.6885, val_acc:0.5374\n",
      "iteration 174 – lstm_size:180, dropout:0.4, learning_rate:0.0055, epochs:4, loss:0.6926, acc:0.5242, val_loss:0.6888, val_acc:0.522\n",
      "iteration 175 – lstm_size:60, dropout:0.25, learning_rate:0.0045, epochs:6, loss:0.6914, acc:0.5212, val_loss:0.6888, val_acc:0.5286\n",
      "iteration 176 – lstm_size:70, dropout:0.15, learning_rate:0.004, epochs:11, loss:0.6903, acc:0.5293, val_loss:0.6908, val_acc:0.4956\n",
      "iteration 177 – lstm_size:80, dropout:0.15, learning_rate:0.0065, epochs:12, loss:0.69, acc:0.5306, val_loss:0.6884, val_acc:0.5286\n",
      "iteration 178 – lstm_size:110, dropout:0.25, learning_rate:0.01, epochs:5, loss:0.6916, acc:0.5301, val_loss:0.6882, val_acc:0.533\n",
      "iteration 179 – lstm_size:160, dropout:0.3, learning_rate:0.0085, epochs:1, loss:0.6958, acc:0.5046, val_loss:0.6867, val_acc:0.5198\n",
      "iteration 180 – lstm_size:180, dropout:0, learning_rate:0.005, epochs:7, loss:0.6937, acc:0.5226, val_loss:0.6884, val_acc:0.5396\n",
      "iteration 181 – lstm_size:120, dropout:0.4, learning_rate:0.0045, epochs:5, loss:0.6919, acc:0.529, val_loss:0.6894, val_acc:0.511\n",
      "iteration 182 – lstm_size:50, dropout:0.3, learning_rate:0.01, epochs:2, loss:0.6928, acc:0.5263, val_loss:0.6909, val_acc:0.5154\n",
      "iteration 183 – lstm_size:60, dropout:0.15, learning_rate:0.009, epochs:12, loss:0.69, acc:0.5253, val_loss:0.6817, val_acc:0.5485\n",
      "iteration 184 – lstm_size:140, dropout:0.3, learning_rate:0.007, epochs:5, loss:0.6918, acc:0.5204, val_loss:0.6913, val_acc:0.522\n",
      "iteration 185 – lstm_size:190, dropout:0.05, learning_rate:0.0045, epochs:7, loss:0.6913, acc:0.525, val_loss:0.6892, val_acc:0.522\n",
      "iteration 186 – lstm_size:190, dropout:0.05, learning_rate:0.006, epochs:2, loss:0.6925, acc:0.5226, val_loss:0.6879, val_acc:0.5154\n",
      "iteration 187 – lstm_size:120, dropout:0.3, learning_rate:0.0045, epochs:4, loss:0.6921, acc:0.5242, val_loss:0.6896, val_acc:0.5286\n",
      "iteration 188 – lstm_size:180, dropout:0.05, learning_rate:0.005, epochs:6, loss:0.691, acc:0.5253, val_loss:0.6881, val_acc:0.5463\n",
      "iteration 189 – lstm_size:100, dropout:0.1, learning_rate:0.0065, epochs:12, loss:0.6897, acc:0.5376, val_loss:0.6881, val_acc:0.5176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 190 – lstm_size:50, dropout:0.4, learning_rate:0.004, epochs:7, loss:0.6917, acc:0.5279, val_loss:0.6899, val_acc:0.5198\n",
      "iteration 191 – lstm_size:170, dropout:0.1, learning_rate:0.0065, epochs:1, loss:0.6942, acc:0.5191, val_loss:0.6909, val_acc:0.5132\n",
      "iteration 192 – lstm_size:150, dropout:0.35, learning_rate:0.009, epochs:1, loss:0.6966, acc:0.511, val_loss:0.688, val_acc:0.5308\n",
      "iteration 193 – lstm_size:180, dropout:0, learning_rate:0.0095, epochs:5, loss:0.6917, acc:0.525, val_loss:0.6872, val_acc:0.5132\n",
      "iteration 194 – lstm_size:150, dropout:0.05, learning_rate:0.0065, epochs:4, loss:0.6911, acc:0.5285, val_loss:0.688, val_acc:0.5066\n",
      "iteration 195 – lstm_size:130, dropout:0.1, learning_rate:0.0065, epochs:1, loss:0.6938, acc:0.511, val_loss:0.6909, val_acc:0.5088\n",
      "iteration 196 – lstm_size:130, dropout:0.3, learning_rate:0.006, epochs:12, loss:0.6909, acc:0.5253, val_loss:0.6906, val_acc:0.5132\n",
      "iteration 197 – lstm_size:140, dropout:0.05, learning_rate:0.004, epochs:4, loss:0.6915, acc:0.5185, val_loss:0.6899, val_acc:0.5198\n",
      "iteration 198 – lstm_size:90, dropout:0.35, learning_rate:0.0095, epochs:1, loss:0.6951, acc:0.5121, val_loss:0.6904, val_acc:0.5022\n",
      "iteration 199 – lstm_size:180, dropout:0.25, learning_rate:0.0095, epochs:4, loss:0.6921, acc:0.5223, val_loss:0.6904, val_acc:0.5088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 13:16:31.344389 17652 deprecation.py:506] From C:\\Users\\jack_\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0712 13:16:31.345390 17652 deprecation.py:506] From C:\\Users\\jack_\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0712 13:16:31.346391 17652 deprecation.py:506] From C:\\Users\\jack_\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 – lstm_size:60, dropout:0.15, learning_rate:0.0085, epochs:2, loss:0.6925, acc:0.5234, val_loss:0.6913, val_acc:0.5154\n"
     ]
    }
   ],
   "source": [
    "results, best_model = random_search(model_fn, p, 200, \"search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>lstm_size</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.536002</td>\n",
       "      <td>0.20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.691142</td>\n",
       "      <td>200</td>\n",
       "      <td>0.559471</td>\n",
       "      <td>0.684303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.524449</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.690926</td>\n",
       "      <td>140</td>\n",
       "      <td>0.552863</td>\n",
       "      <td>0.685534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.525255</td>\n",
       "      <td>0.15</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.690043</td>\n",
       "      <td>60</td>\n",
       "      <td>0.548458</td>\n",
       "      <td>0.681705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.528479</td>\n",
       "      <td>0.40</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.691596</td>\n",
       "      <td>200</td>\n",
       "      <td>0.548458</td>\n",
       "      <td>0.686424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.534927</td>\n",
       "      <td>0.10</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.686391</td>\n",
       "      <td>90</td>\n",
       "      <td>0.546256</td>\n",
       "      <td>0.687720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.525255</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.690988</td>\n",
       "      <td>180</td>\n",
       "      <td>0.546256</td>\n",
       "      <td>0.688144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.520956</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.690914</td>\n",
       "      <td>90</td>\n",
       "      <td>0.546256</td>\n",
       "      <td>0.688151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.530897</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.691228</td>\n",
       "      <td>110</td>\n",
       "      <td>0.546256</td>\n",
       "      <td>0.685604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.529554</td>\n",
       "      <td>0.20</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.690535</td>\n",
       "      <td>100</td>\n",
       "      <td>0.544053</td>\n",
       "      <td>0.686795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.504299</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.694999</td>\n",
       "      <td>170</td>\n",
       "      <td>0.544053</td>\n",
       "      <td>0.688652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.518001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.693840</td>\n",
       "      <td>160</td>\n",
       "      <td>0.544053</td>\n",
       "      <td>0.687353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.530629</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.691987</td>\n",
       "      <td>190</td>\n",
       "      <td>0.541850</td>\n",
       "      <td>0.688383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.532509</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.690985</td>\n",
       "      <td>110</td>\n",
       "      <td>0.541850</td>\n",
       "      <td>0.686226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.519613</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.693331</td>\n",
       "      <td>120</td>\n",
       "      <td>0.541850</td>\n",
       "      <td>0.688092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.513971</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.692598</td>\n",
       "      <td>190</td>\n",
       "      <td>0.539648</td>\n",
       "      <td>0.689404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.522569</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.693670</td>\n",
       "      <td>180</td>\n",
       "      <td>0.539648</td>\n",
       "      <td>0.688379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.526330</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.691818</td>\n",
       "      <td>180</td>\n",
       "      <td>0.539648</td>\n",
       "      <td>0.688526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.519882</td>\n",
       "      <td>0.30</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.692889</td>\n",
       "      <td>180</td>\n",
       "      <td>0.539648</td>\n",
       "      <td>0.690546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.528479</td>\n",
       "      <td>0.30</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.691118</td>\n",
       "      <td>90</td>\n",
       "      <td>0.539648</td>\n",
       "      <td>0.686812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.522837</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.691468</td>\n",
       "      <td>190</td>\n",
       "      <td>0.539648</td>\n",
       "      <td>0.686319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.517195</td>\n",
       "      <td>0.25</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.691762</td>\n",
       "      <td>170</td>\n",
       "      <td>0.539648</td>\n",
       "      <td>0.688548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.517732</td>\n",
       "      <td>0.40</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.692414</td>\n",
       "      <td>170</td>\n",
       "      <td>0.537445</td>\n",
       "      <td>0.685695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.541107</td>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.690719</td>\n",
       "      <td>50</td>\n",
       "      <td>0.537445</td>\n",
       "      <td>0.689983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.531435</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.692762</td>\n",
       "      <td>130</td>\n",
       "      <td>0.537445</td>\n",
       "      <td>0.687352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.534659</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.690263</td>\n",
       "      <td>70</td>\n",
       "      <td>0.537445</td>\n",
       "      <td>0.688395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.537077</td>\n",
       "      <td>0.25</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.690546</td>\n",
       "      <td>50</td>\n",
       "      <td>0.537445</td>\n",
       "      <td>0.688465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.531166</td>\n",
       "      <td>0.35</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.691989</td>\n",
       "      <td>150</td>\n",
       "      <td>0.537445</td>\n",
       "      <td>0.688643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.520956</td>\n",
       "      <td>0.35</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.691618</td>\n",
       "      <td>50</td>\n",
       "      <td>0.537445</td>\n",
       "      <td>0.688210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.520956</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.693965</td>\n",
       "      <td>90</td>\n",
       "      <td>0.535242</td>\n",
       "      <td>0.687411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.529823</td>\n",
       "      <td>0.35</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.690673</td>\n",
       "      <td>70</td>\n",
       "      <td>0.535242</td>\n",
       "      <td>0.688178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.516926</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.691631</td>\n",
       "      <td>170</td>\n",
       "      <td>0.506608</td>\n",
       "      <td>0.690143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.532509</td>\n",
       "      <td>0.25</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.690981</td>\n",
       "      <td>60</td>\n",
       "      <td>0.506608</td>\n",
       "      <td>0.690748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.528479</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.691089</td>\n",
       "      <td>150</td>\n",
       "      <td>0.506608</td>\n",
       "      <td>0.687983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.524449</td>\n",
       "      <td>0.35</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.691052</td>\n",
       "      <td>150</td>\n",
       "      <td>0.506608</td>\n",
       "      <td>0.689582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.520688</td>\n",
       "      <td>0.40</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.694606</td>\n",
       "      <td>60</td>\n",
       "      <td>0.506608</td>\n",
       "      <td>0.687164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.517732</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.694232</td>\n",
       "      <td>140</td>\n",
       "      <td>0.506608</td>\n",
       "      <td>0.688211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.521225</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.692211</td>\n",
       "      <td>120</td>\n",
       "      <td>0.506608</td>\n",
       "      <td>0.692184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.529823</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.690214</td>\n",
       "      <td>130</td>\n",
       "      <td>0.506608</td>\n",
       "      <td>0.686816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.508598</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.697771</td>\n",
       "      <td>200</td>\n",
       "      <td>0.506608</td>\n",
       "      <td>0.686573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.509941</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.693954</td>\n",
       "      <td>150</td>\n",
       "      <td>0.504405</td>\n",
       "      <td>0.691179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.522300</td>\n",
       "      <td>0.30</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.691389</td>\n",
       "      <td>80</td>\n",
       "      <td>0.504405</td>\n",
       "      <td>0.691521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.516658</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.692587</td>\n",
       "      <td>200</td>\n",
       "      <td>0.504405</td>\n",
       "      <td>0.691560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.500806</td>\n",
       "      <td>0.40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.699251</td>\n",
       "      <td>190</td>\n",
       "      <td>0.504405</td>\n",
       "      <td>0.689325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.524181</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.691944</td>\n",
       "      <td>80</td>\n",
       "      <td>0.504405</td>\n",
       "      <td>0.693006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.517732</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.696038</td>\n",
       "      <td>180</td>\n",
       "      <td>0.504405</td>\n",
       "      <td>0.690484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.519076</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.695341</td>\n",
       "      <td>120</td>\n",
       "      <td>0.504405</td>\n",
       "      <td>0.693048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.512090</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.695120</td>\n",
       "      <td>90</td>\n",
       "      <td>0.502203</td>\n",
       "      <td>0.690448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.523106</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.691679</td>\n",
       "      <td>100</td>\n",
       "      <td>0.502203</td>\n",
       "      <td>0.691835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.518538</td>\n",
       "      <td>0.35</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.691171</td>\n",
       "      <td>90</td>\n",
       "      <td>0.502203</td>\n",
       "      <td>0.691850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.520419</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.693060</td>\n",
       "      <td>80</td>\n",
       "      <td>0.502203</td>\n",
       "      <td>0.693001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.524987</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.691812</td>\n",
       "      <td>60</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.691446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.518538</td>\n",
       "      <td>0.35</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.692863</td>\n",
       "      <td>80</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.693187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.523643</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.691873</td>\n",
       "      <td>90</td>\n",
       "      <td>0.497797</td>\n",
       "      <td>0.691824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.529285</td>\n",
       "      <td>0.15</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.690307</td>\n",
       "      <td>70</td>\n",
       "      <td>0.495595</td>\n",
       "      <td>0.690774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.526867</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.691997</td>\n",
       "      <td>190</td>\n",
       "      <td>0.493392</td>\n",
       "      <td>0.692752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.528211</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.691290</td>\n",
       "      <td>120</td>\n",
       "      <td>0.491189</td>\n",
       "      <td>0.690819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.519344</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.693638</td>\n",
       "      <td>180</td>\n",
       "      <td>0.488987</td>\n",
       "      <td>0.691845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.519344</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.694354</td>\n",
       "      <td>200</td>\n",
       "      <td>0.488987</td>\n",
       "      <td>0.692309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.505373</td>\n",
       "      <td>0.35</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.695835</td>\n",
       "      <td>180</td>\n",
       "      <td>0.488987</td>\n",
       "      <td>0.692611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.527673</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.691550</td>\n",
       "      <td>70</td>\n",
       "      <td>0.488987</td>\n",
       "      <td>0.691726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          acc  dropout  epochs  learning_rate      loss  lstm_size   val_acc  \\\n",
       "125  0.536002     0.20      10         0.0100  0.691142        200  0.559471   \n",
       "103  0.524449     0.25       6         0.0100  0.690926        140  0.552863   \n",
       "182  0.525255     0.15      12         0.0090  0.690043         60  0.548458   \n",
       "89   0.528479     0.40       3         0.0055  0.691596        200  0.548458   \n",
       "54   0.534927     0.10      19         0.0045  0.686391         90  0.546256   \n",
       "187  0.525255     0.05       6         0.0050  0.690988        180  0.546256   \n",
       "2    0.520956     0.10       5         0.0100  0.690914         90  0.546256   \n",
       "40   0.530897     0.15       6         0.0100  0.691228        110  0.546256   \n",
       "102  0.529554     0.20       7         0.0090  0.690535        100  0.544053   \n",
       "119  0.504299     0.40       1         0.0065  0.694999        170  0.544053   \n",
       "53   0.518001     0.25       2         0.0080  0.693840        160  0.544053   \n",
       "165  0.530629     0.15       3         0.0095  0.691987        190  0.541850   \n",
       "78   0.532509     0.25       6         0.0045  0.690985        110  0.541850   \n",
       "121  0.519613     0.00       1         0.0055  0.693331        120  0.541850   \n",
       "47   0.513971     0.30       2         0.0045  0.692598        190  0.539648   \n",
       "179  0.522569     0.00       7         0.0050  0.693670        180  0.539648   \n",
       "45   0.526330     0.05       6         0.0075  0.691818        180  0.539648   \n",
       "104  0.519882     0.30       5         0.0100  0.692889        180  0.539648   \n",
       "30   0.528479     0.30       6         0.0080  0.691118         90  0.539648   \n",
       "28   0.522837     0.20      13         0.0080  0.691468        190  0.539648   \n",
       "108  0.517195     0.25      11         0.0085  0.691762        170  0.539648   \n",
       "10   0.517732     0.40       3         0.0070  0.692414        170  0.537445   \n",
       "97   0.541107     0.15       7         0.0090  0.690719         50  0.537445   \n",
       "56   0.531435     0.25       2         0.0070  0.692762        130  0.537445   \n",
       "20   0.534659     0.00      11         0.0070  0.690263         70  0.537445   \n",
       "172  0.537077     0.25       9         0.0085  0.690546         50  0.537445   \n",
       "132  0.531166     0.35      10         0.0090  0.691989        150  0.537445   \n",
       "158  0.520956     0.35       4         0.0090  0.691618         50  0.537445   \n",
       "80   0.520956     0.00       1         0.0085  0.693965         90  0.535242   \n",
       "11   0.529823     0.35       8         0.0065  0.690673         70  0.535242   \n",
       "..        ...      ...     ...            ...       ...        ...       ...   \n",
       "146  0.516926     0.40      12         0.0090  0.691631        170  0.506608   \n",
       "163  0.532509     0.25       8         0.0085  0.690981         60  0.506608   \n",
       "193  0.528479     0.05       4         0.0065  0.691089        150  0.506608   \n",
       "37   0.524449     0.35       6         0.0045  0.691052        150  0.506608   \n",
       "35   0.520688     0.40       7         0.0075  0.694606         60  0.506608   \n",
       "57   0.517732     0.25       1         0.0075  0.694232        140  0.506608   \n",
       "73   0.521225     0.00       1         0.0045  0.692211        120  0.506608   \n",
       "24   0.529823     0.30       7         0.0065  0.690214        130  0.506608   \n",
       "107  0.508598     0.20       1         0.0100  0.697771        200  0.506608   \n",
       "92   0.509941     0.00       4         0.0090  0.693954        150  0.504405   \n",
       "22   0.522300     0.30       5         0.0065  0.691389         80  0.504405   \n",
       "70   0.516658     0.15       3         0.0075  0.692587        200  0.504405   \n",
       "168  0.500806     0.40       8         0.0090  0.699251        190  0.504405   \n",
       "143  0.524181     0.25       2         0.0100  0.691944         80  0.504405   \n",
       "69   0.517732     0.10       4         0.0090  0.696038        180  0.504405   \n",
       "137  0.519076     0.10       1         0.0080  0.695341        120  0.504405   \n",
       "197  0.512090     0.35       1         0.0095  0.695120         90  0.502203   \n",
       "110  0.523106     0.00       3         0.0050  0.691679        100  0.502203   \n",
       "130  0.518538     0.35       3         0.0100  0.691171         90  0.502203   \n",
       "155  0.520419     0.40       2         0.0040  0.693060         80  0.502203   \n",
       "150  0.524987     0.20       2         0.0075  0.691812         60  0.500000   \n",
       "115  0.518538     0.35       2         0.0045  0.692863         80  0.500000   \n",
       "55   0.523643     0.05       2         0.0050  0.691873         90  0.497797   \n",
       "175  0.529285     0.15      11         0.0040  0.690307         70  0.495595   \n",
       "87   0.526867     0.20       5         0.0080  0.691997        190  0.493392   \n",
       "111  0.528211     0.25       4         0.0060  0.691290        120  0.491189   \n",
       "16   0.519344     0.15       1         0.0040  0.693638        180  0.488987   \n",
       "59   0.519344     0.10       1         0.0055  0.694354        200  0.488987   \n",
       "68   0.505373     0.35       5         0.0045  0.695835        180  0.488987   \n",
       "105  0.527673     0.05       6         0.0045  0.691550         70  0.488987   \n",
       "\n",
       "     val_loss  \n",
       "125  0.684303  \n",
       "103  0.685534  \n",
       "182  0.681705  \n",
       "89   0.686424  \n",
       "54   0.687720  \n",
       "187  0.688144  \n",
       "2    0.688151  \n",
       "40   0.685604  \n",
       "102  0.686795  \n",
       "119  0.688652  \n",
       "53   0.687353  \n",
       "165  0.688383  \n",
       "78   0.686226  \n",
       "121  0.688092  \n",
       "47   0.689404  \n",
       "179  0.688379  \n",
       "45   0.688526  \n",
       "104  0.690546  \n",
       "30   0.686812  \n",
       "28   0.686319  \n",
       "108  0.688548  \n",
       "10   0.685695  \n",
       "97   0.689983  \n",
       "56   0.687352  \n",
       "20   0.688395  \n",
       "172  0.688465  \n",
       "132  0.688643  \n",
       "158  0.688210  \n",
       "80   0.687411  \n",
       "11   0.688178  \n",
       "..        ...  \n",
       "146  0.690143  \n",
       "163  0.690748  \n",
       "193  0.687983  \n",
       "37   0.689582  \n",
       "35   0.687164  \n",
       "57   0.688211  \n",
       "73   0.692184  \n",
       "24   0.686816  \n",
       "107  0.686573  \n",
       "92   0.691179  \n",
       "22   0.691521  \n",
       "70   0.691560  \n",
       "168  0.689325  \n",
       "143  0.693006  \n",
       "69   0.690484  \n",
       "137  0.693048  \n",
       "197  0.690448  \n",
       "110  0.691835  \n",
       "130  0.691850  \n",
       "155  0.693001  \n",
       "150  0.691446  \n",
       "115  0.693187  \n",
       "55   0.691824  \n",
       "175  0.690774  \n",
       "87   0.692752  \n",
       "111  0.690819  \n",
       "16   0.691845  \n",
       "59   0.692309  \n",
       "68   0.692611  \n",
       "105  0.691726  \n",
       "\n",
       "[200 rows x 8 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(\"val_acc\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6983279064297676, 0.5132743]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate_generator(test_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
