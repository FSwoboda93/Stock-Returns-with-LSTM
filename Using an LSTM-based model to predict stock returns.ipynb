{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an LSTM-based model to predict stock returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define training, validation and test periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date = \"2008-01-01\"\n",
    "train_end_date = \"2016-12-31\"\n",
    "val_start_date = \"2017-01-01\"\n",
    "val_end_date = \"2017-12-31\"\n",
    "test_start_date = \"2018-01-01\"\n",
    "test_end_date = \"2018-12-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read and label the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezj = pd.read_csv(\"ezj.csv\", index_col=0, parse_dates=True)\n",
    "ezj[\"return\"] = ezj[\"close\"] / ezj[\"close\"].shift() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-01-02</th>\n",
       "      <td>664.908997</td>\n",
       "      <td>677.455017</td>\n",
       "      <td>654.544983</td>\n",
       "      <td>657.273010</td>\n",
       "      <td>1721833.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-03</th>\n",
       "      <td>651.273010</td>\n",
       "      <td>654.000000</td>\n",
       "      <td>617.455017</td>\n",
       "      <td>632.182007</td>\n",
       "      <td>2740650.0</td>\n",
       "      <td>-0.038174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-04</th>\n",
       "      <td>627.817993</td>\n",
       "      <td>632.726990</td>\n",
       "      <td>590.726990</td>\n",
       "      <td>596.726990</td>\n",
       "      <td>4711938.0</td>\n",
       "      <td>-0.056084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-07</th>\n",
       "      <td>596.182007</td>\n",
       "      <td>597.817993</td>\n",
       "      <td>560.726990</td>\n",
       "      <td>583.091003</td>\n",
       "      <td>4103622.0</td>\n",
       "      <td>-0.022851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-08</th>\n",
       "      <td>567.273010</td>\n",
       "      <td>573.273010</td>\n",
       "      <td>497.782013</td>\n",
       "      <td>502.091003</td>\n",
       "      <td>12687374.0</td>\n",
       "      <td>-0.138915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  open        high         low       close      volume  \\\n",
       "date                                                                     \n",
       "2008-01-02  664.908997  677.455017  654.544983  657.273010   1721833.0   \n",
       "2008-01-03  651.273010  654.000000  617.455017  632.182007   2740650.0   \n",
       "2008-01-04  627.817993  632.726990  590.726990  596.726990   4711938.0   \n",
       "2008-01-07  596.182007  597.817993  560.726990  583.091003   4103622.0   \n",
       "2008-01-08  567.273010  573.273010  497.782013  502.091003  12687374.0   \n",
       "\n",
       "              return  \n",
       "date                  \n",
       "2008-01-02       NaN  \n",
       "2008-01-03 -0.038174  \n",
       "2008-01-04 -0.056084  \n",
       "2008-01-07 -0.022851  \n",
       "2008-01-08 -0.138915  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ezj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezj[\"label\"] = np.where(ezj[\"return\"] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Engineer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezj[\"std_return\"] = (ezj[\"return\"] - ezj[\"return\"][:val_start_date].mean()) / ezj[\"return\"][:val_start_date].std()\n",
    "ezj[\"std_volume\"] = (ezj[\"volume\"] - ezj[\"volume\"].rolling(50).mean()) / ezj[\"volume\"].rolling(50).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezj.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_start_iloc = ezj.index.get_loc(val_start_date, method=\"bfill\")\n",
    "test_start_iloc = ezj.index.get_loc(test_start_date, method=\"bfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = TimeseriesGenerator(ezj[[\"std_return\", \"std_volume\"]].values, ezj[[\"label\"]].values,\n",
    "                                      length=30, batch_size=64, end_index=val_start_iloc-1)\n",
    "val_generator = TimeseriesGenerator(ezj[[\"std_return\", \"std_volume\"]].values, ezj[[\"label\"]].values,\n",
    "                                    length=30, batch_size=64, start_index=val_start_iloc,\n",
    "                                    end_index=test_start_iloc-1)\n",
    "test_generator = TimeseriesGenerator(ezj[[\"std_return\", \"std_volume\"]].values, ezj[[\"label\"]].values,\n",
    "                                     length=30, batch_size=64, start_index=test_start_iloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create `model_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(params):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.CuDNNLSTM(params[\"lstm_size\"], input_shape=(30, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(params[\"dropout\"]))\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(params[\"learning_rate\"]),\n",
    "                  loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=5,\n",
    "                                                  restore_best_weights=True)]\n",
    "    history = model.fit_generator(train_generator, validation_data=val_generator,\n",
    "                                  callbacks=callbacks, epochs=100, verbose=0).history\n",
    "    return (history, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create `random_search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(model_fn, search_space, n_iter, search_dir):\n",
    "    results = []\n",
    "    os.mkdir(search_dir)\n",
    "    best_model_path = os.path.join(search_dir, \"best_model.h5\")\n",
    "    results_path = os.path.join(search_dir, \"results.csv\")\n",
    "    for i in range(n_iter):\n",
    "        params = {k: v[np.random.randint(len(v))] for k, v in search_space.items()}\n",
    "        history, model = model_fn(params)\n",
    "        epochs = np.argmax(history[\"val_acc\"]) + 1\n",
    "        result = {k: v[epochs - 1] for k, v in history.items()}\n",
    "        params[\"epochs\"] = epochs\n",
    "        if i == 0:\n",
    "            best_val_acc = result[\"val_acc\"]\n",
    "            model.save(best_model_path)\n",
    "        if result[\"val_acc\"] > best_val_acc:\n",
    "            best_val_acc = result[\"val_acc\"]\n",
    "            model.save(best_model_path)\n",
    "        result = {**params, **result}\n",
    "        results.append(result)\n",
    "        tf.keras.backend.clear_session()\n",
    "        print(f\"iteration {i + 1} â€“ {', '.join(f'{k}:{v:.4g}' for k, v in result.items())}\")\n",
    "    best_model = tf.keras.models.load_model(best_model_path)\n",
    "    results = pd.DataFrame(results)\n",
    "    results.to_csv(results_path)\n",
    "    return (results, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\"lstm_size\": np.linspace(50, 200, 16, dtype=int),\n",
    "                \"dropout\": np.linspace(0, 0.4, 9),\n",
    "                \"learning_rate\": np.linspace(0.004, 0.01, 13)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 â€“ lstm_size:140, dropout:0.2, learning_rate:0.0055, epochs:3, loss:0.6913, acc:0.5271, val_loss:0.6921, val_acc:0.5225\n",
      "iteration 2 â€“ lstm_size:80, dropout:0.15, learning_rate:0.0045, epochs:9, loss:0.6868, acc:0.5367, val_loss:0.6936, val_acc:0.5631\n",
      "iteration 3 â€“ lstm_size:60, dropout:0.2, learning_rate:0.0045, epochs:3, loss:0.691, acc:0.5285, val_loss:0.6944, val_acc:0.527\n",
      "iteration 4 â€“ lstm_size:130, dropout:0.2, learning_rate:0.0065, epochs:3, loss:0.6921, acc:0.5308, val_loss:0.6922, val_acc:0.545\n",
      "iteration 5 â€“ lstm_size:160, dropout:0.25, learning_rate:0.0095, epochs:8, loss:0.6871, acc:0.5426, val_loss:0.6911, val_acc:0.527\n",
      "iteration 6 â€“ lstm_size:80, dropout:0, learning_rate:0.009, epochs:4, loss:0.6899, acc:0.5298, val_loss:0.6894, val_acc:0.5495\n",
      "iteration 7 â€“ lstm_size:120, dropout:0.1, learning_rate:0.0075, epochs:7, loss:0.6882, acc:0.5308, val_loss:0.6926, val_acc:0.5315\n",
      "iteration 8 â€“ lstm_size:130, dropout:0.4, learning_rate:0.005, epochs:2, loss:0.691, acc:0.5308, val_loss:0.7125, val_acc:0.5495\n",
      "iteration 9 â€“ lstm_size:170, dropout:0.4, learning_rate:0.0065, epochs:3, loss:0.6926, acc:0.5244, val_loss:0.6917, val_acc:0.5541\n",
      "iteration 10 â€“ lstm_size:120, dropout:0.05, learning_rate:0.0055, epochs:3, loss:0.6909, acc:0.5276, val_loss:0.6938, val_acc:0.545\n",
      "iteration 11 â€“ lstm_size:110, dropout:0.3, learning_rate:0.008, epochs:2, loss:0.6912, acc:0.5244, val_loss:0.6928, val_acc:0.536\n",
      "iteration 12 â€“ lstm_size:60, dropout:0.3, learning_rate:0.0055, epochs:3, loss:0.6919, acc:0.5394, val_loss:0.694, val_acc:0.482\n",
      "iteration 13 â€“ lstm_size:190, dropout:0.4, learning_rate:0.0065, epochs:4, loss:0.6921, acc:0.5244, val_loss:0.6935, val_acc:0.527\n",
      "iteration 14 â€“ lstm_size:100, dropout:0.4, learning_rate:0.009, epochs:4, loss:0.6917, acc:0.5244, val_loss:0.6916, val_acc:0.536\n",
      "iteration 15 â€“ lstm_size:100, dropout:0.25, learning_rate:0.0095, epochs:6, loss:0.6883, acc:0.5349, val_loss:0.6924, val_acc:0.5586\n",
      "iteration 16 â€“ lstm_size:110, dropout:0.1, learning_rate:0.007, epochs:1, loss:0.6996, acc:0.513, val_loss:0.6914, val_acc:0.5405\n",
      "iteration 17 â€“ lstm_size:140, dropout:0.15, learning_rate:0.0095, epochs:9, loss:0.6884, acc:0.5403, val_loss:0.7034, val_acc:0.545\n",
      "iteration 18 â€“ lstm_size:180, dropout:0.35, learning_rate:0.0045, epochs:7, loss:0.6883, acc:0.5453, val_loss:0.69, val_acc:0.545\n",
      "iteration 19 â€“ lstm_size:110, dropout:0.15, learning_rate:0.009, epochs:1, loss:0.6952, acc:0.5198, val_loss:0.6951, val_acc:0.5225\n",
      "iteration 20 â€“ lstm_size:130, dropout:0.25, learning_rate:0.004, epochs:8, loss:0.6892, acc:0.5385, val_loss:0.6919, val_acc:0.5541\n",
      "iteration 21 â€“ lstm_size:180, dropout:0.3, learning_rate:0.008, epochs:10, loss:0.6865, acc:0.5367, val_loss:0.6929, val_acc:0.5405\n",
      "iteration 22 â€“ lstm_size:90, dropout:0.25, learning_rate:0.005, epochs:1, loss:0.6956, acc:0.5198, val_loss:0.6901, val_acc:0.5495\n",
      "iteration 23 â€“ lstm_size:190, dropout:0.35, learning_rate:0.0085, epochs:2, loss:0.6947, acc:0.5084, val_loss:0.6879, val_acc:0.5495\n",
      "iteration 24 â€“ lstm_size:160, dropout:0.2, learning_rate:0.0065, epochs:8, loss:0.6869, acc:0.5444, val_loss:0.693, val_acc:0.536\n",
      "iteration 25 â€“ lstm_size:140, dropout:0.25, learning_rate:0.0055, epochs:5, loss:0.69, acc:0.5276, val_loss:0.6974, val_acc:0.5541\n",
      "iteration 26 â€“ lstm_size:200, dropout:0, learning_rate:0.005, epochs:7, loss:0.6893, acc:0.5426, val_loss:0.6907, val_acc:0.527\n",
      "iteration 27 â€“ lstm_size:160, dropout:0.15, learning_rate:0.007, epochs:3, loss:0.6928, acc:0.5103, val_loss:0.699, val_acc:0.4955\n",
      "iteration 28 â€“ lstm_size:120, dropout:0.25, learning_rate:0.0045, epochs:1, loss:0.696, acc:0.5134, val_loss:0.6931, val_acc:0.5045\n",
      "iteration 29 â€“ lstm_size:150, dropout:0.1, learning_rate:0.0045, epochs:3, loss:0.6901, acc:0.5403, val_loss:0.689, val_acc:0.536\n",
      "iteration 30 â€“ lstm_size:190, dropout:0.4, learning_rate:0.008, epochs:1, loss:0.7017, acc:0.5039, val_loss:0.6917, val_acc:0.545\n",
      "iteration 31 â€“ lstm_size:190, dropout:0.1, learning_rate:0.01, epochs:7, loss:0.6884, acc:0.5449, val_loss:0.6876, val_acc:0.536\n",
      "iteration 32 â€“ lstm_size:200, dropout:0.4, learning_rate:0.009, epochs:2, loss:0.6922, acc:0.518, val_loss:0.6894, val_acc:0.5315\n",
      "iteration 33 â€“ lstm_size:120, dropout:0.35, learning_rate:0.005, epochs:6, loss:0.6905, acc:0.5276, val_loss:0.6932, val_acc:0.545\n",
      "iteration 34 â€“ lstm_size:90, dropout:0.3, learning_rate:0.0095, epochs:4, loss:0.6914, acc:0.5267, val_loss:0.6913, val_acc:0.5586\n",
      "iteration 35 â€“ lstm_size:170, dropout:0.15, learning_rate:0.009, epochs:1, loss:0.7031, acc:0.5207, val_loss:0.6904, val_acc:0.5495\n",
      "iteration 36 â€“ lstm_size:150, dropout:0.3, learning_rate:0.0045, epochs:6, loss:0.6924, acc:0.5317, val_loss:0.6907, val_acc:0.5315\n",
      "iteration 37 â€“ lstm_size:110, dropout:0.2, learning_rate:0.0095, epochs:9, loss:0.6892, acc:0.518, val_loss:0.6957, val_acc:0.545\n",
      "iteration 38 â€“ lstm_size:100, dropout:0.4, learning_rate:0.008, epochs:4, loss:0.6907, acc:0.5289, val_loss:0.6931, val_acc:0.5315\n",
      "iteration 39 â€“ lstm_size:160, dropout:0.25, learning_rate:0.0045, epochs:1, loss:0.6941, acc:0.5116, val_loss:0.6907, val_acc:0.5225\n",
      "iteration 40 â€“ lstm_size:90, dropout:0.15, learning_rate:0.0055, epochs:4, loss:0.6899, acc:0.5326, val_loss:0.6932, val_acc:0.5405\n",
      "iteration 41 â€“ lstm_size:190, dropout:0.25, learning_rate:0.007, epochs:2, loss:0.6938, acc:0.5175, val_loss:0.6916, val_acc:0.5315\n",
      "iteration 42 â€“ lstm_size:120, dropout:0.4, learning_rate:0.009, epochs:4, loss:0.6908, acc:0.5335, val_loss:0.6923, val_acc:0.5495\n",
      "iteration 43 â€“ lstm_size:180, dropout:0.4, learning_rate:0.005, epochs:1, loss:0.6976, acc:0.4998, val_loss:0.6912, val_acc:0.5495\n",
      "iteration 44 â€“ lstm_size:140, dropout:0.15, learning_rate:0.0055, epochs:3, loss:0.6917, acc:0.5157, val_loss:0.6897, val_acc:0.5495\n",
      "iteration 45 â€“ lstm_size:100, dropout:0.4, learning_rate:0.007, epochs:1, loss:0.696, acc:0.5157, val_loss:0.6908, val_acc:0.5495\n",
      "iteration 46 â€“ lstm_size:80, dropout:0.35, learning_rate:0.0075, epochs:5, loss:0.6907, acc:0.5376, val_loss:0.6916, val_acc:0.5405\n",
      "iteration 47 â€“ lstm_size:70, dropout:0.3, learning_rate:0.0065, epochs:1, loss:0.6966, acc:0.4975, val_loss:0.6903, val_acc:0.545\n",
      "iteration 48 â€“ lstm_size:80, dropout:0.1, learning_rate:0.005, epochs:7, loss:0.6893, acc:0.5349, val_loss:0.6898, val_acc:0.527\n",
      "iteration 49 â€“ lstm_size:110, dropout:0.05, learning_rate:0.0055, epochs:10, loss:0.6875, acc:0.5339, val_loss:0.6892, val_acc:0.5315\n",
      "iteration 50 â€“ lstm_size:130, dropout:0.25, learning_rate:0.009, epochs:1, loss:0.7026, acc:0.5071, val_loss:0.6922, val_acc:0.5405\n",
      "iteration 51 â€“ lstm_size:190, dropout:0.4, learning_rate:0.0065, epochs:1, loss:0.6972, acc:0.5007, val_loss:0.6892, val_acc:0.527\n",
      "iteration 52 â€“ lstm_size:200, dropout:0.1, learning_rate:0.005, epochs:10, loss:0.6871, acc:0.5344, val_loss:0.6938, val_acc:0.527\n",
      "iteration 53 â€“ lstm_size:180, dropout:0.2, learning_rate:0.0055, epochs:1, loss:0.6962, acc:0.5226, val_loss:0.6929, val_acc:0.5315\n",
      "iteration 54 â€“ lstm_size:80, dropout:0.15, learning_rate:0.005, epochs:9, loss:0.6874, acc:0.5467, val_loss:0.6919, val_acc:0.536\n",
      "iteration 55 â€“ lstm_size:50, dropout:0.05, learning_rate:0.007, epochs:4, loss:0.6898, acc:0.5326, val_loss:0.6942, val_acc:0.527\n",
      "iteration 56 â€“ lstm_size:100, dropout:0, learning_rate:0.008, epochs:1, loss:0.6945, acc:0.5221, val_loss:0.6912, val_acc:0.518\n",
      "iteration 57 â€“ lstm_size:160, dropout:0, learning_rate:0.0085, epochs:3, loss:0.6897, acc:0.5239, val_loss:0.69, val_acc:0.545\n",
      "iteration 58 â€“ lstm_size:110, dropout:0.15, learning_rate:0.004, epochs:2, loss:0.691, acc:0.5358, val_loss:0.6904, val_acc:0.5631\n",
      "iteration 59 â€“ lstm_size:190, dropout:0.35, learning_rate:0.005, epochs:8, loss:0.6924, acc:0.5426, val_loss:0.6934, val_acc:0.5631\n",
      "iteration 60 â€“ lstm_size:100, dropout:0.3, learning_rate:0.0095, epochs:6, loss:0.6908, acc:0.5285, val_loss:0.6918, val_acc:0.5405\n",
      "iteration 61 â€“ lstm_size:200, dropout:0.35, learning_rate:0.006, epochs:7, loss:0.6889, acc:0.5444, val_loss:0.6964, val_acc:0.518\n",
      "iteration 62 â€“ lstm_size:90, dropout:0.2, learning_rate:0.009, epochs:2, loss:0.6924, acc:0.523, val_loss:0.6904, val_acc:0.5586\n",
      "iteration 63 â€“ lstm_size:190, dropout:0.1, learning_rate:0.005, epochs:3, loss:0.6908, acc:0.5339, val_loss:0.6885, val_acc:0.5315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 64 â€“ lstm_size:190, dropout:0.25, learning_rate:0.0045, epochs:1, loss:0.6963, acc:0.528, val_loss:0.6921, val_acc:0.5135\n",
      "iteration 65 â€“ lstm_size:180, dropout:0.2, learning_rate:0.007, epochs:1, loss:0.6977, acc:0.5166, val_loss:0.6973, val_acc:0.5135\n",
      "iteration 66 â€“ lstm_size:140, dropout:0.1, learning_rate:0.004, epochs:4, loss:0.6912, acc:0.5326, val_loss:0.694, val_acc:0.518\n",
      "iteration 67 â€“ lstm_size:80, dropout:0.2, learning_rate:0.0065, epochs:3, loss:0.6917, acc:0.513, val_loss:0.6925, val_acc:0.518\n",
      "iteration 68 â€“ lstm_size:80, dropout:0.1, learning_rate:0.009, epochs:6, loss:0.6893, acc:0.5394, val_loss:0.6905, val_acc:0.5315\n",
      "iteration 69 â€“ lstm_size:90, dropout:0.35, learning_rate:0.005, epochs:3, loss:0.6912, acc:0.5226, val_loss:0.6927, val_acc:0.5405\n",
      "iteration 70 â€“ lstm_size:70, dropout:0.25, learning_rate:0.005, epochs:2, loss:0.6914, acc:0.5358, val_loss:0.6903, val_acc:0.5541\n",
      "iteration 71 â€“ lstm_size:140, dropout:0.2, learning_rate:0.004, epochs:6, loss:0.6898, acc:0.5308, val_loss:0.6907, val_acc:0.5586\n",
      "iteration 72 â€“ lstm_size:110, dropout:0, learning_rate:0.0095, epochs:4, loss:0.691, acc:0.5262, val_loss:0.6912, val_acc:0.5631\n",
      "iteration 73 â€“ lstm_size:150, dropout:0.05, learning_rate:0.0075, epochs:3, loss:0.6916, acc:0.538, val_loss:0.6912, val_acc:0.5225\n",
      "iteration 74 â€“ lstm_size:90, dropout:0.05, learning_rate:0.01, epochs:1, loss:0.6972, acc:0.5162, val_loss:0.6924, val_acc:0.536\n",
      "iteration 75 â€“ lstm_size:50, dropout:0.2, learning_rate:0.005, epochs:9, loss:0.6907, acc:0.5289, val_loss:0.6901, val_acc:0.5541\n",
      "iteration 76 â€“ lstm_size:110, dropout:0.2, learning_rate:0.0065, epochs:2, loss:0.6924, acc:0.5239, val_loss:0.6927, val_acc:0.5495\n",
      "iteration 77 â€“ lstm_size:160, dropout:0.3, learning_rate:0.0055, epochs:2, loss:0.692, acc:0.528, val_loss:0.6927, val_acc:0.5586\n",
      "iteration 78 â€“ lstm_size:100, dropout:0.1, learning_rate:0.004, epochs:5, loss:0.6906, acc:0.5312, val_loss:0.6918, val_acc:0.5541\n",
      "iteration 79 â€“ lstm_size:130, dropout:0.25, learning_rate:0.0095, epochs:4, loss:0.6907, acc:0.5298, val_loss:0.6905, val_acc:0.545\n",
      "iteration 80 â€“ lstm_size:130, dropout:0.4, learning_rate:0.0055, epochs:11, loss:0.6859, acc:0.5339, val_loss:0.6933, val_acc:0.5541\n",
      "iteration 81 â€“ lstm_size:80, dropout:0.4, learning_rate:0.007, epochs:1, loss:0.6973, acc:0.5194, val_loss:0.692, val_acc:0.509\n",
      "iteration 82 â€“ lstm_size:150, dropout:0.15, learning_rate:0.0065, epochs:6, loss:0.6908, acc:0.533, val_loss:0.6866, val_acc:0.5495\n",
      "iteration 83 â€“ lstm_size:160, dropout:0.35, learning_rate:0.0095, epochs:3, loss:0.6932, acc:0.5244, val_loss:0.6931, val_acc:0.545\n",
      "iteration 84 â€“ lstm_size:180, dropout:0.3, learning_rate:0.008, epochs:2, loss:0.6928, acc:0.5303, val_loss:0.6927, val_acc:0.5315\n",
      "iteration 85 â€“ lstm_size:120, dropout:0.1, learning_rate:0.004, epochs:6, loss:0.6891, acc:0.5267, val_loss:0.6904, val_acc:0.527\n",
      "iteration 86 â€“ lstm_size:70, dropout:0.35, learning_rate:0.009, epochs:2, loss:0.6918, acc:0.5189, val_loss:0.6916, val_acc:0.5541\n",
      "iteration 87 â€“ lstm_size:120, dropout:0.25, learning_rate:0.0075, epochs:8, loss:0.6884, acc:0.5267, val_loss:0.6907, val_acc:0.545\n",
      "iteration 88 â€“ lstm_size:170, dropout:0.15, learning_rate:0.005, epochs:4, loss:0.6918, acc:0.5153, val_loss:0.691, val_acc:0.536\n",
      "iteration 89 â€“ lstm_size:100, dropout:0.2, learning_rate:0.0045, epochs:4, loss:0.6907, acc:0.5239, val_loss:0.6925, val_acc:0.5225\n",
      "iteration 90 â€“ lstm_size:50, dropout:0, learning_rate:0.0095, epochs:9, loss:0.6847, acc:0.5449, val_loss:0.7021, val_acc:0.518\n",
      "iteration 91 â€“ lstm_size:190, dropout:0.2, learning_rate:0.009, epochs:3, loss:0.6905, acc:0.5125, val_loss:0.6898, val_acc:0.5811\n",
      "iteration 92 â€“ lstm_size:190, dropout:0.3, learning_rate:0.005, epochs:1, loss:0.6975, acc:0.5148, val_loss:0.6928, val_acc:0.5315\n",
      "iteration 93 â€“ lstm_size:180, dropout:0.3, learning_rate:0.008, epochs:4, loss:0.6926, acc:0.533, val_loss:0.6887, val_acc:0.536\n",
      "iteration 94 â€“ lstm_size:160, dropout:0.1, learning_rate:0.006, epochs:3, loss:0.6903, acc:0.5344, val_loss:0.6905, val_acc:0.536\n",
      "iteration 95 â€“ lstm_size:130, dropout:0, learning_rate:0.008, epochs:4, loss:0.6915, acc:0.5349, val_loss:0.6943, val_acc:0.5315\n",
      "iteration 96 â€“ lstm_size:90, dropout:0.35, learning_rate:0.0065, epochs:3, loss:0.6931, acc:0.5125, val_loss:0.6904, val_acc:0.545\n",
      "iteration 97 â€“ lstm_size:50, dropout:0.05, learning_rate:0.006, epochs:2, loss:0.6915, acc:0.528, val_loss:0.6913, val_acc:0.518\n",
      "iteration 98 â€“ lstm_size:190, dropout:0.2, learning_rate:0.01, epochs:1, loss:0.6951, acc:0.5098, val_loss:0.6902, val_acc:0.5586\n",
      "iteration 99 â€“ lstm_size:100, dropout:0.05, learning_rate:0.0045, epochs:7, loss:0.6878, acc:0.5394, val_loss:0.6988, val_acc:0.5045\n",
      "iteration 100 â€“ lstm_size:140, dropout:0.35, learning_rate:0.006, epochs:1, loss:0.6961, acc:0.5175, val_loss:0.6919, val_acc:0.5135\n",
      "iteration 101 â€“ lstm_size:80, dropout:0.35, learning_rate:0.008, epochs:2, loss:0.6928, acc:0.5093, val_loss:0.6894, val_acc:0.5766\n",
      "iteration 102 â€“ lstm_size:100, dropout:0.2, learning_rate:0.008, epochs:2, loss:0.6923, acc:0.5185, val_loss:0.6894, val_acc:0.5766\n",
      "iteration 103 â€“ lstm_size:150, dropout:0.25, learning_rate:0.0095, epochs:1, loss:0.6955, acc:0.5262, val_loss:0.6892, val_acc:0.5586\n",
      "iteration 104 â€“ lstm_size:190, dropout:0.35, learning_rate:0.0055, epochs:1, loss:0.6968, acc:0.5198, val_loss:0.6949, val_acc:0.518\n",
      "iteration 105 â€“ lstm_size:140, dropout:0.15, learning_rate:0.0085, epochs:2, loss:0.6917, acc:0.5185, val_loss:0.6926, val_acc:0.545\n",
      "iteration 106 â€“ lstm_size:130, dropout:0.35, learning_rate:0.0085, epochs:10, loss:0.6876, acc:0.5317, val_loss:0.6936, val_acc:0.545\n",
      "iteration 107 â€“ lstm_size:190, dropout:0.4, learning_rate:0.0065, epochs:5, loss:0.6924, acc:0.5216, val_loss:0.6879, val_acc:0.5631\n",
      "iteration 108 â€“ lstm_size:200, dropout:0.1, learning_rate:0.006, epochs:6, loss:0.6897, acc:0.5226, val_loss:0.6931, val_acc:0.5541\n",
      "iteration 109 â€“ lstm_size:200, dropout:0, learning_rate:0.0075, epochs:9, loss:0.6862, acc:0.5399, val_loss:0.6918, val_acc:0.5405\n",
      "iteration 110 â€“ lstm_size:100, dropout:0.35, learning_rate:0.0095, epochs:9, loss:0.6858, acc:0.5426, val_loss:0.697, val_acc:0.5315\n",
      "iteration 111 â€“ lstm_size:180, dropout:0.15, learning_rate:0.01, epochs:8, loss:0.7031, acc:0.5198, val_loss:0.6923, val_acc:0.5586\n",
      "iteration 112 â€“ lstm_size:60, dropout:0.35, learning_rate:0.006, epochs:2, loss:0.6925, acc:0.5121, val_loss:0.6893, val_acc:0.5315\n",
      "iteration 113 â€“ lstm_size:110, dropout:0.25, learning_rate:0.01, epochs:3, loss:0.6921, acc:0.5408, val_loss:0.6929, val_acc:0.536\n",
      "iteration 114 â€“ lstm_size:180, dropout:0, learning_rate:0.008, epochs:2, loss:0.6928, acc:0.5207, val_loss:0.6881, val_acc:0.5541\n",
      "iteration 115 â€“ lstm_size:70, dropout:0.35, learning_rate:0.0085, epochs:6, loss:0.6908, acc:0.5362, val_loss:0.6927, val_acc:0.5676\n",
      "iteration 116 â€“ lstm_size:100, dropout:0.2, learning_rate:0.0095, epochs:2, loss:0.6911, acc:0.5335, val_loss:0.6908, val_acc:0.5495\n",
      "iteration 117 â€“ lstm_size:50, dropout:0.25, learning_rate:0.005, epochs:10, loss:0.6889, acc:0.5417, val_loss:0.6898, val_acc:0.5541\n",
      "iteration 118 â€“ lstm_size:150, dropout:0.1, learning_rate:0.008, epochs:6, loss:0.6891, acc:0.5408, val_loss:0.6914, val_acc:0.5405\n",
      "iteration 119 â€“ lstm_size:70, dropout:0, learning_rate:0.01, epochs:2, loss:0.6917, acc:0.5339, val_loss:0.6919, val_acc:0.545\n",
      "iteration 120 â€“ lstm_size:100, dropout:0.15, learning_rate:0.004, epochs:1, loss:0.6948, acc:0.5134, val_loss:0.6941, val_acc:0.518\n",
      "iteration 121 â€“ lstm_size:60, dropout:0.25, learning_rate:0.008, epochs:3, loss:0.6927, acc:0.5098, val_loss:0.692, val_acc:0.527\n",
      "iteration 122 â€“ lstm_size:130, dropout:0.05, learning_rate:0.005, epochs:10, loss:0.6846, acc:0.5517, val_loss:0.696, val_acc:0.536\n",
      "iteration 123 â€“ lstm_size:120, dropout:0.1, learning_rate:0.0095, epochs:3, loss:0.6916, acc:0.5312, val_loss:0.691, val_acc:0.536\n",
      "iteration 124 â€“ lstm_size:100, dropout:0.2, learning_rate:0.0095, epochs:4, loss:0.6971, acc:0.5212, val_loss:0.6892, val_acc:0.5586\n",
      "iteration 125 â€“ lstm_size:100, dropout:0.05, learning_rate:0.004, epochs:6, loss:0.6901, acc:0.5289, val_loss:0.6897, val_acc:0.545\n",
      "iteration 126 â€“ lstm_size:90, dropout:0.4, learning_rate:0.007, epochs:2, loss:0.6903, acc:0.5271, val_loss:0.6892, val_acc:0.5495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 127 â€“ lstm_size:190, dropout:0.25, learning_rate:0.0075, epochs:18, loss:0.6575, acc:0.5859, val_loss:0.7417, val_acc:0.545\n",
      "iteration 128 â€“ lstm_size:150, dropout:0.25, learning_rate:0.008, epochs:10, loss:0.6842, acc:0.549, val_loss:0.695, val_acc:0.5315\n",
      "iteration 129 â€“ lstm_size:130, dropout:0.35, learning_rate:0.007, epochs:3, loss:0.6913, acc:0.5244, val_loss:0.6912, val_acc:0.5541\n",
      "iteration 130 â€“ lstm_size:60, dropout:0.3, learning_rate:0.01, epochs:1, loss:0.6953, acc:0.503, val_loss:0.6933, val_acc:0.5\n",
      "iteration 131 â€“ lstm_size:190, dropout:0.25, learning_rate:0.006, epochs:2, loss:0.6928, acc:0.5335, val_loss:0.6928, val_acc:0.5405\n",
      "iteration 132 â€“ lstm_size:160, dropout:0.4, learning_rate:0.0045, epochs:2, loss:0.6934, acc:0.5198, val_loss:0.6911, val_acc:0.545\n",
      "iteration 133 â€“ lstm_size:130, dropout:0.1, learning_rate:0.0085, epochs:2, loss:0.6925, acc:0.5153, val_loss:0.6909, val_acc:0.509\n",
      "iteration 134 â€“ lstm_size:50, dropout:0.2, learning_rate:0.005, epochs:1, loss:0.6942, acc:0.5166, val_loss:0.6933, val_acc:0.5\n",
      "iteration 135 â€“ lstm_size:50, dropout:0.3, learning_rate:0.008, epochs:1, loss:0.6956, acc:0.5153, val_loss:0.6924, val_acc:0.545\n",
      "iteration 136 â€“ lstm_size:80, dropout:0, learning_rate:0.008, epochs:2, loss:0.6925, acc:0.5189, val_loss:0.6921, val_acc:0.527\n",
      "iteration 137 â€“ lstm_size:80, dropout:0.15, learning_rate:0.006, epochs:5, loss:0.6895, acc:0.5285, val_loss:0.6915, val_acc:0.527\n",
      "iteration 138 â€“ lstm_size:170, dropout:0.2, learning_rate:0.0055, epochs:16, loss:0.678, acc:0.5513, val_loss:0.704, val_acc:0.545\n",
      "iteration 139 â€“ lstm_size:120, dropout:0.05, learning_rate:0.0065, epochs:6, loss:0.6907, acc:0.528, val_loss:0.6898, val_acc:0.5541\n",
      "iteration 140 â€“ lstm_size:100, dropout:0.4, learning_rate:0.005, epochs:1, loss:0.697, acc:0.5103, val_loss:0.6943, val_acc:0.5315\n",
      "iteration 141 â€“ lstm_size:70, dropout:0, learning_rate:0.007, epochs:2, loss:0.6917, acc:0.5262, val_loss:0.6914, val_acc:0.5676\n",
      "iteration 142 â€“ lstm_size:160, dropout:0.3, learning_rate:0.004, epochs:7, loss:0.6908, acc:0.5403, val_loss:0.6896, val_acc:0.545\n",
      "iteration 143 â€“ lstm_size:80, dropout:0.2, learning_rate:0.009, epochs:6, loss:0.6877, acc:0.5554, val_loss:0.692, val_acc:0.5405\n",
      "iteration 144 â€“ lstm_size:190, dropout:0.2, learning_rate:0.009, epochs:4, loss:0.6903, acc:0.5349, val_loss:0.6903, val_acc:0.5405\n",
      "iteration 145 â€“ lstm_size:70, dropout:0.05, learning_rate:0.0095, epochs:2, loss:0.6921, acc:0.5212, val_loss:0.6907, val_acc:0.5541\n",
      "iteration 146 â€“ lstm_size:120, dropout:0.4, learning_rate:0.005, epochs:2, loss:0.6929, acc:0.5248, val_loss:0.6909, val_acc:0.5631\n",
      "iteration 147 â€“ lstm_size:140, dropout:0.15, learning_rate:0.006, epochs:1, loss:0.6972, acc:0.5226, val_loss:0.6914, val_acc:0.536\n",
      "iteration 148 â€“ lstm_size:110, dropout:0.1, learning_rate:0.0045, epochs:6, loss:0.6887, acc:0.5326, val_loss:0.694, val_acc:0.518\n",
      "iteration 149 â€“ lstm_size:110, dropout:0, learning_rate:0.007, epochs:3, loss:0.691, acc:0.5162, val_loss:0.6899, val_acc:0.5631\n",
      "iteration 150 â€“ lstm_size:130, dropout:0.4, learning_rate:0.0095, epochs:5, loss:0.6933, acc:0.5244, val_loss:0.6921, val_acc:0.5586\n",
      "iteration 151 â€“ lstm_size:120, dropout:0.25, learning_rate:0.01, epochs:4, loss:0.6919, acc:0.5339, val_loss:0.6902, val_acc:0.5495\n",
      "iteration 152 â€“ lstm_size:200, dropout:0.3, learning_rate:0.0045, epochs:5, loss:0.6908, acc:0.5312, val_loss:0.6986, val_acc:0.518\n",
      "iteration 153 â€“ lstm_size:170, dropout:0.1, learning_rate:0.008, epochs:7, loss:0.6903, acc:0.5403, val_loss:0.6893, val_acc:0.545\n",
      "iteration 154 â€“ lstm_size:150, dropout:0.05, learning_rate:0.0045, epochs:5, loss:0.6902, acc:0.539, val_loss:0.6908, val_acc:0.5405\n",
      "iteration 155 â€“ lstm_size:170, dropout:0, learning_rate:0.004, epochs:4, loss:0.6914, acc:0.528, val_loss:0.6933, val_acc:0.536\n",
      "iteration 156 â€“ lstm_size:200, dropout:0.05, learning_rate:0.0085, epochs:3, loss:0.69, acc:0.5312, val_loss:0.693, val_acc:0.5405\n",
      "iteration 157 â€“ lstm_size:110, dropout:0.25, learning_rate:0.0075, epochs:14, loss:0.675, acc:0.5699, val_loss:0.6999, val_acc:0.536\n",
      "iteration 158 â€“ lstm_size:50, dropout:0.15, learning_rate:0.008, epochs:4, loss:0.6905, acc:0.5257, val_loss:0.6903, val_acc:0.527\n",
      "iteration 159 â€“ lstm_size:140, dropout:0.3, learning_rate:0.005, epochs:3, loss:0.692, acc:0.5294, val_loss:0.6924, val_acc:0.536\n",
      "iteration 160 â€“ lstm_size:200, dropout:0.2, learning_rate:0.009, epochs:2, loss:0.6935, acc:0.5203, val_loss:0.691, val_acc:0.5405\n",
      "iteration 161 â€“ lstm_size:100, dropout:0.15, learning_rate:0.006, epochs:2, loss:0.6928, acc:0.5267, val_loss:0.6924, val_acc:0.536\n",
      "iteration 162 â€“ lstm_size:100, dropout:0.2, learning_rate:0.0085, epochs:2, loss:0.6923, acc:0.5235, val_loss:0.6898, val_acc:0.5541\n",
      "iteration 163 â€“ lstm_size:200, dropout:0.35, learning_rate:0.008, epochs:6, loss:0.691, acc:0.5335, val_loss:0.6913, val_acc:0.527\n",
      "iteration 164 â€“ lstm_size:190, dropout:0, learning_rate:0.01, epochs:1, loss:0.7053, acc:0.4916, val_loss:0.6901, val_acc:0.5495\n",
      "iteration 165 â€“ lstm_size:50, dropout:0.15, learning_rate:0.0055, epochs:4, loss:0.6901, acc:0.5262, val_loss:0.6924, val_acc:0.536\n",
      "iteration 166 â€“ lstm_size:110, dropout:0.05, learning_rate:0.0075, epochs:7, loss:0.6869, acc:0.5444, val_loss:0.6945, val_acc:0.545\n",
      "iteration 167 â€“ lstm_size:150, dropout:0.05, learning_rate:0.008, epochs:5, loss:0.6898, acc:0.5399, val_loss:0.6922, val_acc:0.5225\n",
      "iteration 168 â€“ lstm_size:170, dropout:0.05, learning_rate:0.0055, epochs:2, loss:0.6916, acc:0.5362, val_loss:0.6925, val_acc:0.536\n",
      "iteration 169 â€“ lstm_size:130, dropout:0.2, learning_rate:0.0075, epochs:6, loss:0.6907, acc:0.5226, val_loss:0.6904, val_acc:0.5541\n",
      "iteration 170 â€“ lstm_size:50, dropout:0.35, learning_rate:0.0095, epochs:8, loss:0.6888, acc:0.523, val_loss:0.6928, val_acc:0.5541\n",
      "iteration 171 â€“ lstm_size:100, dropout:0.1, learning_rate:0.0085, epochs:1, loss:0.6969, acc:0.4966, val_loss:0.6915, val_acc:0.536\n",
      "iteration 172 â€“ lstm_size:120, dropout:0.2, learning_rate:0.0095, epochs:2, loss:0.6936, acc:0.5166, val_loss:0.6938, val_acc:0.5586\n",
      "iteration 173 â€“ lstm_size:70, dropout:0.2, learning_rate:0.005, epochs:5, loss:0.6904, acc:0.5185, val_loss:0.694, val_acc:0.5315\n",
      "iteration 174 â€“ lstm_size:50, dropout:0.25, learning_rate:0.0065, epochs:3, loss:0.6914, acc:0.5203, val_loss:0.694, val_acc:0.5225\n",
      "iteration 175 â€“ lstm_size:160, dropout:0, learning_rate:0.0095, epochs:5, loss:0.6913, acc:0.5289, val_loss:0.6899, val_acc:0.545\n",
      "iteration 176 â€“ lstm_size:120, dropout:0.4, learning_rate:0.01, epochs:3, loss:0.6915, acc:0.5203, val_loss:0.6971, val_acc:0.4955\n",
      "iteration 177 â€“ lstm_size:190, dropout:0.2, learning_rate:0.006, epochs:1, loss:0.696, acc:0.5043, val_loss:0.69, val_acc:0.545\n",
      "iteration 178 â€“ lstm_size:160, dropout:0.05, learning_rate:0.007, epochs:8, loss:0.6869, acc:0.5472, val_loss:0.6941, val_acc:0.5495\n",
      "iteration 179 â€“ lstm_size:90, dropout:0.1, learning_rate:0.005, epochs:1, loss:0.6974, acc:0.4961, val_loss:0.6943, val_acc:0.509\n",
      "iteration 180 â€“ lstm_size:150, dropout:0.1, learning_rate:0.007, epochs:6, loss:0.6895, acc:0.5376, val_loss:0.6913, val_acc:0.5405\n",
      "iteration 181 â€“ lstm_size:120, dropout:0.1, learning_rate:0.006, epochs:1, loss:0.6946, acc:0.5171, val_loss:0.6911, val_acc:0.545\n",
      "iteration 182 â€“ lstm_size:180, dropout:0.05, learning_rate:0.008, epochs:3, loss:0.6916, acc:0.5312, val_loss:0.6901, val_acc:0.5541\n",
      "iteration 183 â€“ lstm_size:60, dropout:0.2, learning_rate:0.0055, epochs:1, loss:0.6969, acc:0.5021, val_loss:0.6909, val_acc:0.5495\n",
      "iteration 184 â€“ lstm_size:70, dropout:0.25, learning_rate:0.005, epochs:6, loss:0.6891, acc:0.533, val_loss:0.6957, val_acc:0.5495\n",
      "iteration 185 â€“ lstm_size:60, dropout:0.15, learning_rate:0.0085, epochs:3, loss:0.6906, acc:0.5248, val_loss:0.6936, val_acc:0.527\n",
      "iteration 186 â€“ lstm_size:120, dropout:0.25, learning_rate:0.0075, epochs:2, loss:0.6927, acc:0.5198, val_loss:0.6921, val_acc:0.5315\n",
      "iteration 187 â€“ lstm_size:90, dropout:0.15, learning_rate:0.005, epochs:2, loss:0.6918, acc:0.533, val_loss:0.6901, val_acc:0.5495\n",
      "iteration 188 â€“ lstm_size:90, dropout:0, learning_rate:0.0075, epochs:5, loss:0.6896, acc:0.5367, val_loss:0.6922, val_acc:0.518\n",
      "iteration 189 â€“ lstm_size:140, dropout:0.05, learning_rate:0.006, epochs:5, loss:0.6898, acc:0.5253, val_loss:0.6897, val_acc:0.5315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 190 â€“ lstm_size:200, dropout:0.2, learning_rate:0.006, epochs:3, loss:0.6903, acc:0.5317, val_loss:0.6892, val_acc:0.527\n",
      "iteration 191 â€“ lstm_size:170, dropout:0, learning_rate:0.0075, epochs:3, loss:0.6913, acc:0.5317, val_loss:0.6925, val_acc:0.5225\n",
      "iteration 192 â€“ lstm_size:130, dropout:0.05, learning_rate:0.009, epochs:2, loss:0.6923, acc:0.5198, val_loss:0.6913, val_acc:0.5315\n",
      "iteration 193 â€“ lstm_size:50, dropout:0, learning_rate:0.0055, epochs:4, loss:0.689, acc:0.538, val_loss:0.6912, val_acc:0.536\n",
      "iteration 194 â€“ lstm_size:120, dropout:0.05, learning_rate:0.01, epochs:1, loss:0.6942, acc:0.5358, val_loss:0.6914, val_acc:0.5495\n",
      "iteration 195 â€“ lstm_size:150, dropout:0.3, learning_rate:0.0095, epochs:1, loss:0.6978, acc:0.5116, val_loss:0.6923, val_acc:0.5045\n",
      "iteration 196 â€“ lstm_size:60, dropout:0.05, learning_rate:0.0085, epochs:1, loss:0.6944, acc:0.5025, val_loss:0.6929, val_acc:0.5631\n",
      "iteration 197 â€“ lstm_size:170, dropout:0.35, learning_rate:0.007, epochs:1, loss:0.7025, acc:0.5062, val_loss:0.6915, val_acc:0.5495\n",
      "iteration 198 â€“ lstm_size:60, dropout:0.05, learning_rate:0.005, epochs:4, loss:0.6897, acc:0.5362, val_loss:0.6921, val_acc:0.5225\n",
      "iteration 199 â€“ lstm_size:90, dropout:0.25, learning_rate:0.007, epochs:5, loss:0.6889, acc:0.5458, val_loss:0.6943, val_acc:0.527\n",
      "iteration 200 â€“ lstm_size:150, dropout:0.25, learning_rate:0.007, epochs:4, loss:0.6908, acc:0.538, val_loss:0.694, val_acc:0.545\n"
     ]
    }
   ],
   "source": [
    "results, best_model = random_search(model_fn, search_space, 200, \"search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>lstm_size</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.512528</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.690524</td>\n",
       "      <td>190</td>\n",
       "      <td>0.581081</td>\n",
       "      <td>0.689752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.509339</td>\n",
       "      <td>0.35</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.692828</td>\n",
       "      <td>80</td>\n",
       "      <td>0.576577</td>\n",
       "      <td>0.689374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.518451</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.692348</td>\n",
       "      <td>100</td>\n",
       "      <td>0.576577</td>\n",
       "      <td>0.689398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.536219</td>\n",
       "      <td>0.35</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.690760</td>\n",
       "      <td>70</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.692731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.526196</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.691741</td>\n",
       "      <td>70</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.691383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          acc  dropout  epochs  learning_rate      loss  lstm_size   val_acc  \\\n",
       "90   0.512528     0.20       3         0.0090  0.690524        190  0.581081   \n",
       "100  0.509339     0.35       2         0.0080  0.692828         80  0.576577   \n",
       "101  0.518451     0.20       2         0.0080  0.692348        100  0.576577   \n",
       "114  0.536219     0.35       6         0.0085  0.690760         70  0.567568   \n",
       "140  0.526196     0.00       2         0.0070  0.691741         70  0.567568   \n",
       "\n",
       "     val_loss  \n",
       "90   0.689752  \n",
       "100  0.689374  \n",
       "101  0.689398  \n",
       "114  0.692731  \n",
       "140  0.691383  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(\"val_acc\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6878005266189575, 0.5515695]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate_generator(test_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
